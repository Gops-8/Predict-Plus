{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler=MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df= pd.read_pickle(\"final_train_test_risk_score.pkl.bz2\",compression=\"bz2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop([\"improvement\",\"member_id\"],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=df.drop(columns=\"score\",axis=1)\n",
    "y=df[\"score\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test ,Train split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, train_size=0.7,test_size=0.3,random_state=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = scaler.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = scaler.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras \n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization, Activation ,Flatten\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_5 (Dense)              (None, 512)               9728      \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 272,897\n",
      "Trainable params: 272,897\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "NN_model = Sequential()\n",
    "\n",
    "# The Input Layer :\n",
    "NN_model.add(Dense(512, kernel_initializer='normal',input_dim = X_train.shape[1], activation='relu'))\n",
    "\n",
    "# The Hidden Layers :\n",
    "NN_model.add(Dense(256, kernel_initializer='normal',activation='relu'))\n",
    "NN_model.add(Dense(256, kernel_initializer='normal',activation='relu'))\n",
    "NN_model.add(Dense(256, kernel_initializer='normal',activation='relu'))\n",
    "\n",
    "# The Output Layer :\n",
    "NN_model.add(Dense(1, kernel_initializer='normal',activation='linear'))\n",
    "\n",
    "# Compile the network :\n",
    "NN_model.compile(loss='mean_absolute_error', optimizer='adam', metrics=['mean_absolute_error'])\n",
    "NN_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_name = 'Weights-{epoch:03d}--{val_loss:.5f}.hdf5' \n",
    "checkpoint = ModelCheckpoint(checkpoint_name, monitor='val_loss', verbose = 1, save_best_only = True, mode ='auto')\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "643/660 [============================>.] - ETA: 0s - loss: 27.2438 - mean_absolute_error: 27.2438\n",
      "Epoch 00001: val_loss improved from inf to 13.07715, saving model to Weights-001--13.07715.hdf5\n",
      "660/660 [==============================] - 2s 3ms/step - loss: 26.8971 - mean_absolute_error: 26.8971 - val_loss: 13.0772 - val_mean_absolute_error: 13.0772\n",
      "Epoch 2/500\n",
      "652/660 [============================>.] - ETA: 0s - loss: 12.3147 - mean_absolute_error: 12.3147\n",
      "Epoch 00002: val_loss improved from 13.07715 to 10.35611, saving model to Weights-002--10.35611.hdf5\n",
      "660/660 [==============================] - 2s 2ms/step - loss: 12.3052 - mean_absolute_error: 12.3052 - val_loss: 10.3561 - val_mean_absolute_error: 10.3561\n",
      "Epoch 3/500\n",
      "650/660 [============================>.] - ETA: 0s - loss: 10.9909 - mean_absolute_error: 10.9909\n",
      "Epoch 00003: val_loss improved from 10.35611 to 10.17166, saving model to Weights-003--10.17166.hdf5\n",
      "660/660 [==============================] - 2s 2ms/step - loss: 10.9692 - mean_absolute_error: 10.9692 - val_loss: 10.1717 - val_mean_absolute_error: 10.1717\n",
      "Epoch 4/500\n",
      "651/660 [============================>.] - ETA: 0s - loss: 10.2498 - mean_absolute_error: 10.2498\n",
      "Epoch 00004: val_loss improved from 10.17166 to 8.99952, saving model to Weights-004--8.99952.hdf5\n",
      "660/660 [==============================] - 2s 2ms/step - loss: 10.2545 - mean_absolute_error: 10.2545 - val_loss: 8.9995 - val_mean_absolute_error: 8.9995\n",
      "Epoch 5/500\n",
      "651/660 [============================>.] - ETA: 0s - loss: 9.7128 - mean_absolute_error: 9.7128\n",
      "Epoch 00005: val_loss did not improve from 8.99952\n",
      "660/660 [==============================] - 2s 2ms/step - loss: 9.6985 - mean_absolute_error: 9.6985 - val_loss: 10.3572 - val_mean_absolute_error: 10.3572\n",
      "Epoch 6/500\n",
      "654/660 [============================>.] - ETA: 0s - loss: 9.4166 - mean_absolute_error: 9.4166\n",
      "Epoch 00006: val_loss improved from 8.99952 to 8.65704, saving model to Weights-006--8.65704.hdf5\n",
      "660/660 [==============================] - 2s 2ms/step - loss: 9.4179 - mean_absolute_error: 9.4179 - val_loss: 8.6570 - val_mean_absolute_error: 8.6570\n",
      "Epoch 7/500\n",
      "655/660 [============================>.] - ETA: 0s - loss: 9.1495 - mean_absolute_error: 9.1495\n",
      "Epoch 00007: val_loss did not improve from 8.65704\n",
      "660/660 [==============================] - 2s 2ms/step - loss: 9.1419 - mean_absolute_error: 9.1419 - val_loss: 8.6965 - val_mean_absolute_error: 8.6965\n",
      "Epoch 8/500\n",
      "649/660 [============================>.] - ETA: 0s - loss: 9.0086 - mean_absolute_error: 9.0086\n",
      "Epoch 00008: val_loss did not improve from 8.65704\n",
      "660/660 [==============================] - 2s 2ms/step - loss: 9.0312 - mean_absolute_error: 9.0312 - val_loss: 10.5150 - val_mean_absolute_error: 10.5150\n",
      "Epoch 9/500\n",
      "653/660 [============================>.] - ETA: 0s - loss: 8.8736 - mean_absolute_error: 8.8736\n",
      "Epoch 00009: val_loss did not improve from 8.65704\n",
      "660/660 [==============================] - 2s 2ms/step - loss: 8.8765 - mean_absolute_error: 8.8765 - val_loss: 8.9734 - val_mean_absolute_error: 8.9734\n",
      "Epoch 10/500\n",
      "650/660 [============================>.] - ETA: 0s - loss: 8.8431 - mean_absolute_error: 8.8431\n",
      "Epoch 00010: val_loss did not improve from 8.65704\n",
      "660/660 [==============================] - 2s 2ms/step - loss: 8.8238 - mean_absolute_error: 8.8238 - val_loss: 10.0880 - val_mean_absolute_error: 10.0880\n",
      "Epoch 11/500\n",
      "655/660 [============================>.] - ETA: 0s - loss: 8.8074 - mean_absolute_error: 8.8074\n",
      "Epoch 00011: val_loss improved from 8.65704 to 8.35291, saving model to Weights-011--8.35291.hdf5\n",
      "660/660 [==============================] - 2s 2ms/step - loss: 8.8016 - mean_absolute_error: 8.8016 - val_loss: 8.3529 - val_mean_absolute_error: 8.3529\n",
      "Epoch 12/500\n",
      "645/660 [============================>.] - ETA: 0s - loss: 8.6631 - mean_absolute_error: 8.6631\n",
      "Epoch 00012: val_loss improved from 8.35291 to 7.77980, saving model to Weights-012--7.77980.hdf5\n",
      "660/660 [==============================] - 2s 3ms/step - loss: 8.6714 - mean_absolute_error: 8.6714 - val_loss: 7.7798 - val_mean_absolute_error: 7.7798\n",
      "Epoch 13/500\n",
      "647/660 [============================>.] - ETA: 0s - loss: 8.7230 - mean_absolute_error: 8.7230\n",
      "Epoch 00013: val_loss did not improve from 7.77980\n",
      "660/660 [==============================] - 2s 2ms/step - loss: 8.7278 - mean_absolute_error: 8.7278 - val_loss: 7.9816 - val_mean_absolute_error: 7.9816\n",
      "Epoch 14/500\n",
      "660/660 [==============================] - ETA: 0s - loss: 8.7221 - mean_absolute_error: 8.7221\n",
      "Epoch 00014: val_loss did not improve from 7.77980\n",
      "660/660 [==============================] - 2s 2ms/step - loss: 8.7221 - mean_absolute_error: 8.7221 - val_loss: 8.1688 - val_mean_absolute_error: 8.1688\n",
      "Epoch 15/500\n",
      "654/660 [============================>.] - ETA: 0s - loss: 8.5412 - mean_absolute_error: 8.5412\n",
      "Epoch 00015: val_loss did not improve from 7.77980\n",
      "660/660 [==============================] - 2s 2ms/step - loss: 8.5473 - mean_absolute_error: 8.5473 - val_loss: 9.8099 - val_mean_absolute_error: 9.8099\n",
      "Epoch 16/500\n",
      "652/660 [============================>.] - ETA: 0s - loss: 8.4995 - mean_absolute_error: 8.4995\n",
      "Epoch 00016: val_loss did not improve from 7.77980\n",
      "660/660 [==============================] - 2s 2ms/step - loss: 8.5128 - mean_absolute_error: 8.5128 - val_loss: 9.9534 - val_mean_absolute_error: 9.9534\n",
      "Epoch 17/500\n",
      "652/660 [============================>.] - ETA: 0s - loss: 8.4646 - mean_absolute_error: 8.4646\n",
      "Epoch 00017: val_loss improved from 7.77980 to 7.71057, saving model to Weights-017--7.71057.hdf5\n",
      "660/660 [==============================] - 2s 2ms/step - loss: 8.4588 - mean_absolute_error: 8.4588 - val_loss: 7.7106 - val_mean_absolute_error: 7.7106\n",
      "Epoch 18/500\n",
      "654/660 [============================>.] - ETA: 0s - loss: 8.4935 - mean_absolute_error: 8.4935\n",
      "Epoch 00018: val_loss did not improve from 7.71057\n",
      "660/660 [==============================] - 2s 2ms/step - loss: 8.5014 - mean_absolute_error: 8.5014 - val_loss: 7.7670 - val_mean_absolute_error: 7.7670\n",
      "Epoch 19/500\n",
      "656/660 [============================>.] - ETA: 0s - loss: 8.6350 - mean_absolute_error: 8.6350\n",
      "Epoch 00019: val_loss did not improve from 7.71057\n",
      "660/660 [==============================] - 2s 2ms/step - loss: 8.6407 - mean_absolute_error: 8.6407 - val_loss: 8.5964 - val_mean_absolute_error: 8.5964\n",
      "Epoch 20/500\n",
      "650/660 [============================>.] - ETA: 0s - loss: 8.4068 - mean_absolute_error: 8.4068\n",
      "Epoch 00020: val_loss did not improve from 7.71057\n",
      "660/660 [==============================] - 2s 2ms/step - loss: 8.4063 - mean_absolute_error: 8.4063 - val_loss: 9.3491 - val_mean_absolute_error: 9.3491\n",
      "Epoch 21/500\n",
      "652/660 [============================>.] - ETA: 0s - loss: 8.5031 - mean_absolute_error: 8.5031\n",
      "Epoch 00021: val_loss did not improve from 7.71057\n",
      "660/660 [==============================] - 2s 2ms/step - loss: 8.4977 - mean_absolute_error: 8.4977 - val_loss: 7.8437 - val_mean_absolute_error: 7.8437\n",
      "Epoch 22/500\n",
      "653/660 [============================>.] - ETA: 0s - loss: 8.2428 - mean_absolute_error: 8.2428\n",
      "Epoch 00022: val_loss improved from 7.71057 to 7.66714, saving model to Weights-022--7.66714.hdf5\n",
      "660/660 [==============================] - 2s 2ms/step - loss: 8.2584 - mean_absolute_error: 8.2584 - val_loss: 7.6671 - val_mean_absolute_error: 7.6671\n",
      "Epoch 23/500\n",
      "654/660 [============================>.] - ETA: 0s - loss: 8.5248 - mean_absolute_error: 8.5248\n",
      "Epoch 00023: val_loss improved from 7.66714 to 7.52372, saving model to Weights-023--7.52372.hdf5\n",
      "660/660 [==============================] - 2s 2ms/step - loss: 8.5247 - mean_absolute_error: 8.5247 - val_loss: 7.5237 - val_mean_absolute_error: 7.5237\n",
      "Epoch 24/500\n",
      "659/660 [============================>.] - ETA: 0s - loss: 8.3856 - mean_absolute_error: 8.3856\n",
      "Epoch 00024: val_loss did not improve from 7.52372\n",
      "660/660 [==============================] - 2s 2ms/step - loss: 8.3895 - mean_absolute_error: 8.3895 - val_loss: 7.8923 - val_mean_absolute_error: 7.8923\n",
      "Epoch 25/500\n",
      "651/660 [============================>.] - ETA: 0s - loss: 8.5154 - mean_absolute_error: 8.5154\n",
      "Epoch 00025: val_loss did not improve from 7.52372\n",
      "660/660 [==============================] - 2s 2ms/step - loss: 8.5180 - mean_absolute_error: 8.5180 - val_loss: 7.5436 - val_mean_absolute_error: 7.5436\n",
      "Epoch 26/500\n",
      "653/660 [============================>.] - ETA: 0s - loss: 8.3520 - mean_absolute_error: 8.3520\n",
      "Epoch 00026: val_loss did not improve from 7.52372\n",
      "660/660 [==============================] - 2s 2ms/step - loss: 8.3385 - mean_absolute_error: 8.3385 - val_loss: 7.6649 - val_mean_absolute_error: 7.6649\n",
      "Epoch 27/500\n",
      "637/660 [===========================>..] - ETA: 0s - loss: 8.2366 - mean_absolute_error: 8.2366\n",
      "Epoch 00027: val_loss improved from 7.52372 to 7.45419, saving model to Weights-027--7.45419.hdf5\n",
      "660/660 [==============================] - 2s 2ms/step - loss: 8.2179 - mean_absolute_error: 8.2179 - val_loss: 7.4542 - val_mean_absolute_error: 7.4542\n",
      "Epoch 28/500\n",
      "653/660 [============================>.] - ETA: 0s - loss: 8.2617 - mean_absolute_error: 8.2617\n",
      "Epoch 00028: val_loss did not improve from 7.45419\n",
      "660/660 [==============================] - 2s 2ms/step - loss: 8.2686 - mean_absolute_error: 8.2686 - val_loss: 8.8223 - val_mean_absolute_error: 8.8223\n",
      "Epoch 29/500\n",
      "651/660 [============================>.] - ETA: 0s - loss: 8.3302 - mean_absolute_error: 8.3302\n",
      "Epoch 00029: val_loss did not improve from 7.45419\n",
      "660/660 [==============================] - 2s 2ms/step - loss: 8.3177 - mean_absolute_error: 8.3177 - val_loss: 8.5846 - val_mean_absolute_error: 8.5846\n",
      "Epoch 30/500\n",
      "655/660 [============================>.] - ETA: 0s - loss: 8.1657 - mean_absolute_error: 8.1657\n",
      "Epoch 00030: val_loss did not improve from 7.45419\n",
      "660/660 [==============================] - 2s 2ms/step - loss: 8.1630 - mean_absolute_error: 8.1630 - val_loss: 7.5435 - val_mean_absolute_error: 7.5435\n",
      "Epoch 31/500\n",
      "636/660 [===========================>..] - ETA: 0s - loss: 8.3457 - mean_absolute_error: 8.3457\n",
      "Epoch 00031: val_loss did not improve from 7.45419\n",
      "660/660 [==============================] - 2s 2ms/step - loss: 8.3590 - mean_absolute_error: 8.3590 - val_loss: 7.5708 - val_mean_absolute_error: 7.5708\n",
      "Epoch 32/500\n",
      "658/660 [============================>.] - ETA: 0s - loss: 8.0768 - mean_absolute_error: 8.0768\n",
      "Epoch 00032: val_loss did not improve from 7.45419\n",
      "660/660 [==============================] - 2s 2ms/step - loss: 8.0770 - mean_absolute_error: 8.0770 - val_loss: 7.6633 - val_mean_absolute_error: 7.6633\n",
      "Epoch 33/500\n",
      "637/660 [===========================>..] - ETA: 0s - loss: 8.3790 - mean_absolute_error: 8.3790\n",
      "Epoch 00033: val_loss improved from 7.45419 to 7.42180, saving model to Weights-033--7.42180.hdf5\n",
      "660/660 [==============================] - 2s 2ms/step - loss: 8.3449 - mean_absolute_error: 8.3449 - val_loss: 7.4218 - val_mean_absolute_error: 7.4218\n",
      "Epoch 34/500\n",
      "637/660 [===========================>..] - ETA: 0s - loss: 8.2070 - mean_absolute_error: 8.2070\n",
      "Epoch 00034: val_loss did not improve from 7.42180\n",
      "660/660 [==============================] - 2s 2ms/step - loss: 8.2009 - mean_absolute_error: 8.2009 - val_loss: 8.1260 - val_mean_absolute_error: 8.1260\n",
      "Epoch 35/500\n",
      "654/660 [============================>.] - ETA: 0s - loss: 8.1413 - mean_absolute_error: 8.1413\n",
      "Epoch 00035: val_loss did not improve from 7.42180\n",
      "660/660 [==============================] - 2s 2ms/step - loss: 8.1552 - mean_absolute_error: 8.1552 - val_loss: 7.9225 - val_mean_absolute_error: 7.9225\n",
      "Epoch 36/500\n",
      "658/660 [============================>.] - ETA: 0s - loss: 8.3582 - mean_absolute_error: 8.3582\n",
      "Epoch 00036: val_loss did not improve from 7.42180\n",
      "660/660 [==============================] - 2s 2ms/step - loss: 8.3611 - mean_absolute_error: 8.3611 - val_loss: 7.5744 - val_mean_absolute_error: 7.5744\n",
      "Epoch 37/500\n",
      "657/660 [============================>.] - ETA: 0s - loss: 7.9901 - mean_absolute_error: 7.9901\n",
      "Epoch 00037: val_loss did not improve from 7.42180\n",
      "660/660 [==============================] - 2s 2ms/step - loss: 7.9889 - mean_absolute_error: 7.9889 - val_loss: 7.6659 - val_mean_absolute_error: 7.6659\n",
      "Epoch 38/500\n",
      "657/660 [============================>.] - ETA: 0s - loss: 8.1237 - mean_absolute_error: 8.1237\n",
      "Epoch 00038: val_loss did not improve from 7.42180\n",
      "660/660 [==============================] - 2s 2ms/step - loss: 8.1200 - mean_absolute_error: 8.1200 - val_loss: 7.8159 - val_mean_absolute_error: 7.8159\n",
      "Epoch 39/500\n",
      "655/660 [============================>.] - ETA: 0s - loss: 8.0858 - mean_absolute_error: 8.0858\n",
      "Epoch 00039: val_loss did not improve from 7.42180\n",
      "660/660 [==============================] - 2s 2ms/step - loss: 8.0839 - mean_absolute_error: 8.0839 - val_loss: 8.2437 - val_mean_absolute_error: 8.2437\n",
      "Epoch 40/500\n",
      "658/660 [============================>.] - ETA: 0s - loss: 8.2472 - mean_absolute_error: 8.2472\n",
      "Epoch 00040: val_loss improved from 7.42180 to 7.35999, saving model to Weights-040--7.35999.hdf5\n",
      "660/660 [==============================] - 2s 2ms/step - loss: 8.2475 - mean_absolute_error: 8.2475 - val_loss: 7.3600 - val_mean_absolute_error: 7.3600\n",
      "Epoch 41/500\n",
      "657/660 [============================>.] - ETA: 0s - loss: 7.9792 - mean_absolute_error: 7.9792\n",
      "Epoch 00041: val_loss did not improve from 7.35999\n",
      "660/660 [==============================] - 2s 2ms/step - loss: 7.9875 - mean_absolute_error: 7.9875 - val_loss: 8.3332 - val_mean_absolute_error: 8.3332\n",
      "Epoch 42/500\n",
      "654/660 [============================>.] - ETA: 0s - loss: 7.9289 - mean_absolute_error: 7.9289\n",
      "Epoch 00042: val_loss improved from 7.35999 to 7.34495, saving model to Weights-042--7.34495.hdf5\n",
      "660/660 [==============================] - 2s 2ms/step - loss: 7.9284 - mean_absolute_error: 7.9284 - val_loss: 7.3449 - val_mean_absolute_error: 7.3449\n",
      "Epoch 43/500\n",
      "637/660 [===========================>..] - ETA: 0s - loss: 7.9884 - mean_absolute_error: 7.9884\n",
      "Epoch 00043: val_loss did not improve from 7.34495\n",
      "660/660 [==============================] - 1s 2ms/step - loss: 8.0204 - mean_absolute_error: 8.0204 - val_loss: 10.9559 - val_mean_absolute_error: 10.9559\n",
      "Epoch 44/500\n",
      "658/660 [============================>.] - ETA: 0s - loss: 8.0953 - mean_absolute_error: 8.0953\n",
      "Epoch 00044: val_loss improved from 7.34495 to 7.32664, saving model to Weights-044--7.32664.hdf5\n",
      "660/660 [==============================] - 2s 2ms/step - loss: 8.1001 - mean_absolute_error: 8.1001 - val_loss: 7.3266 - val_mean_absolute_error: 7.3266\n",
      "Epoch 45/500\n",
      "653/660 [============================>.] - ETA: 0s - loss: 8.0848 - mean_absolute_error: 8.0848\n",
      "Epoch 00045: val_loss did not improve from 7.32664\n",
      "660/660 [==============================] - 2s 2ms/step - loss: 8.0922 - mean_absolute_error: 8.0922 - val_loss: 7.3990 - val_mean_absolute_error: 7.3990\n",
      "Epoch 46/500\n",
      "654/660 [============================>.] - ETA: 0s - loss: 7.9687 - mean_absolute_error: 7.9687\n",
      "Epoch 00046: val_loss did not improve from 7.32664\n",
      "660/660 [==============================] - 2s 2ms/step - loss: 7.9605 - mean_absolute_error: 7.9605 - val_loss: 7.5527 - val_mean_absolute_error: 7.5527\n",
      "Epoch 47/500\n",
      "655/660 [============================>.] - ETA: 0s - loss: 8.0366 - mean_absolute_error: 8.0366\n",
      "Epoch 00047: val_loss did not improve from 7.32664\n",
      "660/660 [==============================] - 2s 2ms/step - loss: 8.0322 - mean_absolute_error: 8.0322 - val_loss: 8.2883 - val_mean_absolute_error: 8.2883\n",
      "Epoch 48/500\n",
      "654/660 [============================>.] - ETA: 0s - loss: 8.0321 - mean_absolute_error: 8.0321\n",
      "Epoch 00048: val_loss did not improve from 7.32664\n",
      "660/660 [==============================] - 2s 2ms/step - loss: 8.0356 - mean_absolute_error: 8.0356 - val_loss: 7.5233 - val_mean_absolute_error: 7.5233\n",
      "Epoch 49/500\n",
      "654/660 [============================>.] - ETA: 0s - loss: 8.0360 - mean_absolute_error: 8.0360\n",
      "Epoch 00049: val_loss did not improve from 7.32664\n",
      "660/660 [==============================] - 2s 2ms/step - loss: 8.0363 - mean_absolute_error: 8.0363 - val_loss: 7.8681 - val_mean_absolute_error: 7.8681\n",
      "Epoch 50/500\n",
      "657/660 [============================>.] - ETA: 0s - loss: 8.0038 - mean_absolute_error: 8.0038\n",
      "Epoch 00050: val_loss did not improve from 7.32664\n",
      "660/660 [==============================] - 2s 2ms/step - loss: 8.0114 - mean_absolute_error: 8.0114 - val_loss: 8.1416 - val_mean_absolute_error: 8.1416\n",
      "Epoch 51/500\n",
      "651/660 [============================>.] - ETA: 0s - loss: 8.0661 - mean_absolute_error: 8.0661\n",
      "Epoch 00051: val_loss did not improve from 7.32664\n",
      "660/660 [==============================] - 2s 2ms/step - loss: 8.0595 - mean_absolute_error: 8.0595 - val_loss: 7.5769 - val_mean_absolute_error: 7.5769\n",
      "Epoch 52/500\n",
      "650/660 [============================>.] - ETA: 0s - loss: 8.0310 - mean_absolute_error: 8.0310\n",
      "Epoch 00052: val_loss did not improve from 7.32664\n",
      "660/660 [==============================] - 2s 2ms/step - loss: 8.0193 - mean_absolute_error: 8.0193 - val_loss: 9.1442 - val_mean_absolute_error: 9.1442\n",
      "Epoch 53/500\n",
      "658/660 [============================>.] - ETA: 0s - loss: 7.9232 - mean_absolute_error: 7.9232\n",
      "Epoch 00053: val_loss did not improve from 7.32664\n",
      "660/660 [==============================] - 2s 2ms/step - loss: 7.9230 - mean_absolute_error: 7.9230 - val_loss: 7.9889 - val_mean_absolute_error: 7.9889\n",
      "Epoch 54/500\n",
      "652/660 [============================>.] - ETA: 0s - loss: 7.9856 - mean_absolute_error: 7.9856\n",
      "Epoch 00054: val_loss did not improve from 7.32664\n",
      "660/660 [==============================] - 2s 2ms/step - loss: 7.9917 - mean_absolute_error: 7.9917 - val_loss: 7.8363 - val_mean_absolute_error: 7.8363\n",
      "Epoch 55/500\n",
      "652/660 [============================>.] - ETA: 0s - loss: 7.8820 - mean_absolute_error: 7.8820\n",
      "Epoch 00055: val_loss did not improve from 7.32664\n",
      "660/660 [==============================] - 2s 2ms/step - loss: 7.8903 - mean_absolute_error: 7.8903 - val_loss: 7.8959 - val_mean_absolute_error: 7.8959\n",
      "Epoch 56/500\n",
      "652/660 [============================>.] - ETA: 0s - loss: 8.0204 - mean_absolute_error: 8.0204\n",
      "Epoch 00056: val_loss did not improve from 7.32664\n",
      "660/660 [==============================] - 2s 2ms/step - loss: 8.0248 - mean_absolute_error: 8.0248 - val_loss: 7.5622 - val_mean_absolute_error: 7.5622\n",
      "Epoch 57/500\n",
      "651/660 [============================>.] - ETA: 0s - loss: 7.8935 - mean_absolute_error: 7.8935\n",
      "Epoch 00057: val_loss did not improve from 7.32664\n",
      "660/660 [==============================] - 2s 2ms/step - loss: 7.8962 - mean_absolute_error: 7.8962 - val_loss: 7.4831 - val_mean_absolute_error: 7.4831\n",
      "Epoch 58/500\n",
      "653/660 [============================>.] - ETA: 0s - loss: 8.0155 - mean_absolute_error: 8.0155\n",
      "Epoch 00058: val_loss did not improve from 7.32664\n",
      "660/660 [==============================] - 2s 2ms/step - loss: 7.9967 - mean_absolute_error: 7.9967 - val_loss: 8.4792 - val_mean_absolute_error: 8.4792\n",
      "Epoch 59/500\n",
      "653/660 [============================>.] - ETA: 0s - loss: 7.9175 - mean_absolute_error: 7.9175\n",
      "Epoch 00059: val_loss did not improve from 7.32664\n",
      "660/660 [==============================] - 2s 2ms/step - loss: 7.9229 - mean_absolute_error: 7.9229 - val_loss: 9.2027 - val_mean_absolute_error: 9.2027\n",
      "Epoch 60/500\n",
      "652/660 [============================>.] - ETA: 0s - loss: 7.8532 - mean_absolute_error: 7.8532\n",
      "Epoch 00060: val_loss did not improve from 7.32664\n",
      "660/660 [==============================] - 2s 2ms/step - loss: 7.8599 - mean_absolute_error: 7.8599 - val_loss: 7.7307 - val_mean_absolute_error: 7.7307\n",
      "Epoch 61/500\n",
      "652/660 [============================>.] - ETA: 0s - loss: 7.8135 - mean_absolute_error: 7.8135\n",
      "Epoch 00061: val_loss did not improve from 7.32664\n",
      "660/660 [==============================] - 2s 2ms/step - loss: 7.8186 - mean_absolute_error: 7.8186 - val_loss: 8.2302 - val_mean_absolute_error: 8.2302\n",
      "Epoch 62/500\n",
      "651/660 [============================>.] - ETA: 0s - loss: 8.0889 - mean_absolute_error: 8.0889\n",
      "Epoch 00062: val_loss improved from 7.32664 to 7.29240, saving model to Weights-062--7.29240.hdf5\n",
      "660/660 [==============================] - 2s 2ms/step - loss: 8.0691 - mean_absolute_error: 8.0691 - val_loss: 7.2924 - val_mean_absolute_error: 7.2924\n",
      "Epoch 63/500\n",
      "651/660 [============================>.] - ETA: 0s - loss: 7.9366 - mean_absolute_error: 7.9366\n",
      "Epoch 00063: val_loss did not improve from 7.29240\n",
      "660/660 [==============================] - 2s 2ms/step - loss: 7.9291 - mean_absolute_error: 7.9291 - val_loss: 7.5163 - val_mean_absolute_error: 7.5163\n",
      "Epoch 64/500\n",
      "650/660 [============================>.] - ETA: 0s - loss: 7.7752 - mean_absolute_error: 7.7752\n",
      "Epoch 00064: val_loss did not improve from 7.29240\n",
      "660/660 [==============================] - 2s 2ms/step - loss: 7.7806 - mean_absolute_error: 7.7806 - val_loss: 7.3531 - val_mean_absolute_error: 7.3531\n",
      "Epoch 65/500\n",
      "651/660 [============================>.] - ETA: 0s - loss: 7.8602 - mean_absolute_error: 7.8602\n",
      "Epoch 00065: val_loss did not improve from 7.29240\n",
      "660/660 [==============================] - 2s 2ms/step - loss: 7.8664 - mean_absolute_error: 7.8664 - val_loss: 9.0736 - val_mean_absolute_error: 9.0736\n",
      "Epoch 66/500\n",
      "651/660 [============================>.] - ETA: 0s - loss: 7.8669 - mean_absolute_error: 7.8669\n",
      "Epoch 00066: val_loss did not improve from 7.29240\n",
      "660/660 [==============================] - 2s 2ms/step - loss: 7.8702 - mean_absolute_error: 7.8702 - val_loss: 8.0486 - val_mean_absolute_error: 8.0486\n",
      "Epoch 67/500\n",
      "651/660 [============================>.] - ETA: 0s - loss: 7.7783 - mean_absolute_error: 7.7783\n",
      "Epoch 00067: val_loss did not improve from 7.29240\n",
      "660/660 [==============================] - 2s 2ms/step - loss: 7.7634 - mean_absolute_error: 7.7634 - val_loss: 7.8737 - val_mean_absolute_error: 7.8737\n",
      "Epoch 68/500\n",
      "652/660 [============================>.] - ETA: 0s - loss: 7.7775 - mean_absolute_error: 7.7775\n",
      "Epoch 00068: val_loss did not improve from 7.29240\n",
      "660/660 [==============================] - 2s 2ms/step - loss: 7.7740 - mean_absolute_error: 7.7740 - val_loss: 7.6812 - val_mean_absolute_error: 7.6812\n",
      "Epoch 69/500\n",
      "651/660 [============================>.] - ETA: 0s - loss: 7.8521 - mean_absolute_error: 7.8521\n",
      "Epoch 00069: val_loss did not improve from 7.29240\n",
      "660/660 [==============================] - 2s 2ms/step - loss: 7.8462 - mean_absolute_error: 7.8462 - val_loss: 7.5948 - val_mean_absolute_error: 7.5948\n",
      "Epoch 70/500\n",
      "651/660 [============================>.] - ETA: 0s - loss: 7.8079 - mean_absolute_error: 7.8079\n",
      "Epoch 00070: val_loss did not improve from 7.29240\n",
      "660/660 [==============================] - 2s 2ms/step - loss: 7.8119 - mean_absolute_error: 7.8119 - val_loss: 7.6760 - val_mean_absolute_error: 7.6760\n",
      "Epoch 71/500\n",
      "649/660 [============================>.] - ETA: 0s - loss: 7.6942 - mean_absolute_error: 7.6942\n",
      "Epoch 00071: val_loss did not improve from 7.29240\n",
      "660/660 [==============================] - 2s 2ms/step - loss: 7.6844 - mean_absolute_error: 7.6844 - val_loss: 7.3558 - val_mean_absolute_error: 7.3558\n",
      "Epoch 72/500\n",
      "653/660 [============================>.] - ETA: 0s - loss: 7.7658 - mean_absolute_error: 7.7658\n",
      "Epoch 00072: val_loss did not improve from 7.29240\n",
      "660/660 [==============================] - 2s 2ms/step - loss: 7.7642 - mean_absolute_error: 7.7642 - val_loss: 7.6521 - val_mean_absolute_error: 7.6521\n",
      "Epoch 73/500\n",
      "654/660 [============================>.] - ETA: 0s - loss: 7.9158 - mean_absolute_error: 7.9158\n",
      "Epoch 00073: val_loss did not improve from 7.29240\n",
      "660/660 [==============================] - 2s 2ms/step - loss: 7.9060 - mean_absolute_error: 7.9060 - val_loss: 7.5498 - val_mean_absolute_error: 7.5498\n",
      "Epoch 74/500\n",
      "653/660 [============================>.] - ETA: 0s - loss: 7.8501 - mean_absolute_error: 7.8501\n",
      "Epoch 00074: val_loss improved from 7.29240 to 7.25215, saving model to Weights-074--7.25215.hdf5\n",
      "660/660 [==============================] - 2s 2ms/step - loss: 7.8488 - mean_absolute_error: 7.8488 - val_loss: 7.2522 - val_mean_absolute_error: 7.2522\n",
      "Epoch 75/500\n",
      "652/660 [============================>.] - ETA: 0s - loss: 7.8247 - mean_absolute_error: 7.8247\n",
      "Epoch 00075: val_loss did not improve from 7.25215\n",
      "660/660 [==============================] - 2s 2ms/step - loss: 7.8192 - mean_absolute_error: 7.8192 - val_loss: 7.4339 - val_mean_absolute_error: 7.4339\n",
      "Epoch 76/500\n",
      "652/660 [============================>.] - ETA: 0s - loss: 7.9163 - mean_absolute_error: 7.9163\n",
      "Epoch 00076: val_loss did not improve from 7.25215\n",
      "660/660 [==============================] - 2s 2ms/step - loss: 7.9230 - mean_absolute_error: 7.9230 - val_loss: 7.7624 - val_mean_absolute_error: 7.7624\n",
      "Epoch 77/500\n",
      "637/660 [===========================>..] - ETA: 0s - loss: 7.7758 - mean_absolute_error: 7.7758\n",
      "Epoch 00077: val_loss did not improve from 7.25215\n",
      "660/660 [==============================] - 1s 2ms/step - loss: 7.7630 - mean_absolute_error: 7.7630 - val_loss: 9.0675 - val_mean_absolute_error: 9.0675\n",
      "Epoch 78/500\n",
      "657/660 [============================>.] - ETA: 0s - loss: 7.7387 - mean_absolute_error: 7.7387\n",
      "Epoch 00078: val_loss did not improve from 7.25215\n",
      "660/660 [==============================] - 2s 2ms/step - loss: 7.7382 - mean_absolute_error: 7.7382 - val_loss: 7.5327 - val_mean_absolute_error: 7.5327\n",
      "Epoch 79/500\n",
      "642/660 [============================>.] - ETA: 0s - loss: 7.7381 - mean_absolute_error: 7.7381\n",
      "Epoch 00079: val_loss did not improve from 7.25215\n",
      "660/660 [==============================] - 1s 2ms/step - loss: 7.7507 - mean_absolute_error: 7.7507 - val_loss: 7.7979 - val_mean_absolute_error: 7.7979\n",
      "Epoch 80/500\n",
      "658/660 [============================>.] - ETA: 0s - loss: 7.8072 - mean_absolute_error: 7.8072\n",
      "Epoch 00080: val_loss did not improve from 7.25215\n",
      "660/660 [==============================] - 2s 2ms/step - loss: 7.8113 - mean_absolute_error: 7.8113 - val_loss: 7.6266 - val_mean_absolute_error: 7.6266\n",
      "Epoch 81/500\n",
      "659/660 [============================>.] - ETA: 0s - loss: 7.7683 - mean_absolute_error: 7.7683\n",
      "Epoch 00081: val_loss did not improve from 7.25215\n",
      "660/660 [==============================] - 2s 2ms/step - loss: 7.7719 - mean_absolute_error: 7.7719 - val_loss: 8.0412 - val_mean_absolute_error: 8.0412\n",
      "Epoch 82/500\n",
      "654/660 [============================>.] - ETA: 0s - loss: 7.8504 - mean_absolute_error: 7.8504\n",
      "Epoch 00082: val_loss did not improve from 7.25215\n",
      "660/660 [==============================] - 2s 2ms/step - loss: 7.8488 - mean_absolute_error: 7.8488 - val_loss: 8.0120 - val_mean_absolute_error: 8.0120\n",
      "Epoch 83/500\n",
      "650/660 [============================>.] - ETA: 0s - loss: 7.7779 - mean_absolute_error: 7.7779\n",
      "Epoch 00083: val_loss did not improve from 7.25215\n",
      "660/660 [==============================] - 2s 2ms/step - loss: 7.7781 - mean_absolute_error: 7.7781 - val_loss: 8.5726 - val_mean_absolute_error: 8.5726\n",
      "Epoch 84/500\n",
      "657/660 [============================>.] - ETA: 0s - loss: 7.7115 - mean_absolute_error: 7.7115\n",
      "Epoch 00084: val_loss did not improve from 7.25215\n",
      "660/660 [==============================] - 2s 2ms/step - loss: 7.7066 - mean_absolute_error: 7.7066 - val_loss: 8.4249 - val_mean_absolute_error: 8.4249\n",
      "Epoch 85/500\n",
      "639/660 [============================>.] - ETA: 0s - loss: 7.7099 - mean_absolute_error: 7.7099\n",
      "Epoch 00085: val_loss did not improve from 7.25215\n",
      "660/660 [==============================] - 1s 2ms/step - loss: 7.7453 - mean_absolute_error: 7.7453 - val_loss: 9.2702 - val_mean_absolute_error: 9.2702\n",
      "Epoch 86/500\n",
      "639/660 [============================>.] - ETA: 0s - loss: 7.7626 - mean_absolute_error: 7.7626\n",
      "Epoch 00086: val_loss did not improve from 7.25215\n",
      "660/660 [==============================] - 1s 2ms/step - loss: 7.7943 - mean_absolute_error: 7.7943 - val_loss: 7.5696 - val_mean_absolute_error: 7.5696\n",
      "Epoch 87/500\n",
      "646/660 [============================>.] - ETA: 0s - loss: 7.7557 - mean_absolute_error: 7.7557\n",
      "Epoch 00087: val_loss did not improve from 7.25215\n",
      "660/660 [==============================] - 1s 2ms/step - loss: 7.7839 - mean_absolute_error: 7.7839 - val_loss: 8.1964 - val_mean_absolute_error: 8.1964\n",
      "Epoch 88/500\n",
      "659/660 [============================>.] - ETA: 0s - loss: 7.6839 - mean_absolute_error: 7.6839\n",
      "Epoch 00088: val_loss did not improve from 7.25215\n",
      "660/660 [==============================] - 2s 2ms/step - loss: 7.6814 - mean_absolute_error: 7.6814 - val_loss: 7.3944 - val_mean_absolute_error: 7.3944\n",
      "Epoch 89/500\n",
      "638/660 [============================>.] - ETA: 0s - loss: 7.8091 - mean_absolute_error: 7.8091\n",
      "Epoch 00089: val_loss did not improve from 7.25215\n",
      "660/660 [==============================] - 1s 2ms/step - loss: 7.7983 - mean_absolute_error: 7.7983 - val_loss: 7.2876 - val_mean_absolute_error: 7.2876\n",
      "Epoch 90/500\n",
      "651/660 [============================>.] - ETA: 0s - loss: 7.7291 - mean_absolute_error: 7.7291\n",
      "Epoch 00090: val_loss did not improve from 7.25215\n",
      "660/660 [==============================] - 2s 2ms/step - loss: 7.7177 - mean_absolute_error: 7.7177 - val_loss: 7.6736 - val_mean_absolute_error: 7.6736\n",
      "Epoch 91/500\n",
      "649/660 [============================>.] - ETA: 0s - loss: 7.7225 - mean_absolute_error: 7.7225\n",
      "Epoch 00091: val_loss did not improve from 7.25215\n",
      "660/660 [==============================] - 2s 2ms/step - loss: 7.7160 - mean_absolute_error: 7.7160 - val_loss: 7.5400 - val_mean_absolute_error: 7.5400\n",
      "Epoch 92/500\n",
      "660/660 [==============================] - ETA: 0s - loss: 7.7670 - mean_absolute_error: 7.7670\n",
      "Epoch 00092: val_loss did not improve from 7.25215\n",
      "660/660 [==============================] - 1s 2ms/step - loss: 7.7670 - mean_absolute_error: 7.7670 - val_loss: 7.5710 - val_mean_absolute_error: 7.5710\n",
      "Epoch 93/500\n",
      "659/660 [============================>.] - ETA: 0s - loss: 7.7790 - mean_absolute_error: 7.7790\n",
      "Epoch 00093: val_loss did not improve from 7.25215\n",
      "660/660 [==============================] - 1s 2ms/step - loss: 7.7776 - mean_absolute_error: 7.7776 - val_loss: 7.6384 - val_mean_absolute_error: 7.6384\n",
      "Epoch 94/500\n",
      "637/660 [===========================>..] - ETA: 0s - loss: 7.7685 - mean_absolute_error: 7.7685\n",
      "Epoch 00094: val_loss did not improve from 7.25215\n",
      "660/660 [==============================] - 1s 2ms/step - loss: 7.7288 - mean_absolute_error: 7.7288 - val_loss: 7.6874 - val_mean_absolute_error: 7.6874\n",
      "Epoch 95/500\n",
      "638/660 [============================>.] - ETA: 0s - loss: 7.7972 - mean_absolute_error: 7.7972\n",
      "Epoch 00095: val_loss did not improve from 7.25215\n",
      "660/660 [==============================] - 2s 2ms/step - loss: 7.8240 - mean_absolute_error: 7.8240 - val_loss: 7.8742 - val_mean_absolute_error: 7.8742\n",
      "Epoch 96/500\n",
      "643/660 [============================>.] - ETA: 0s - loss: 7.9015 - mean_absolute_error: 7.9015\n",
      "Epoch 00096: val_loss did not improve from 7.25215\n",
      "660/660 [==============================] - 2s 2ms/step - loss: 7.9081 - mean_absolute_error: 7.9081 - val_loss: 7.3117 - val_mean_absolute_error: 7.3117\n",
      "Epoch 97/500\n",
      "645/660 [============================>.] - ETA: 0s - loss: 7.7530 - mean_absolute_error: 7.7530\n",
      "Epoch 00097: val_loss did not improve from 7.25215\n",
      "660/660 [==============================] - 1s 2ms/step - loss: 7.7494 - mean_absolute_error: 7.7494 - val_loss: 8.5912 - val_mean_absolute_error: 8.5912\n",
      "Epoch 98/500\n",
      "638/660 [============================>.] - ETA: 0s - loss: 7.7029 - mean_absolute_error: 7.7029\n",
      "Epoch 00098: val_loss did not improve from 7.25215\n",
      "660/660 [==============================] - 2s 2ms/step - loss: 7.7156 - mean_absolute_error: 7.7156 - val_loss: 7.4207 - val_mean_absolute_error: 7.4207\n",
      "Epoch 99/500\n",
      "658/660 [============================>.] - ETA: 0s - loss: 7.6622 - mean_absolute_error: 7.6622\n",
      "Epoch 00099: val_loss improved from 7.25215 to 7.25215, saving model to Weights-099--7.25215.hdf5\n",
      "660/660 [==============================] - 2s 2ms/step - loss: 7.6557 - mean_absolute_error: 7.6557 - val_loss: 7.2521 - val_mean_absolute_error: 7.2521\n",
      "Epoch 100/500\n",
      "638/660 [============================>.] - ETA: 0s - loss: 7.6439 - mean_absolute_error: 7.6439\n",
      "Epoch 00100: val_loss improved from 7.25215 to 7.20456, saving model to Weights-100--7.20456.hdf5\n",
      "660/660 [==============================] - 2s 2ms/step - loss: 7.6312 - mean_absolute_error: 7.6312 - val_loss: 7.2046 - val_mean_absolute_error: 7.2046\n",
      "Epoch 101/500\n",
      "637/660 [===========================>..] - ETA: 0s - loss: 7.5830 - mean_absolute_error: 7.5830\n",
      "Epoch 00101: val_loss did not improve from 7.20456\n",
      "660/660 [==============================] - 1s 2ms/step - loss: 7.5840 - mean_absolute_error: 7.5840 - val_loss: 7.4557 - val_mean_absolute_error: 7.4557\n",
      "Epoch 102/500\n",
      "642/660 [============================>.] - ETA: 0s - loss: 7.7822 - mean_absolute_error: 7.7822\n",
      "Epoch 00102: val_loss did not improve from 7.20456\n",
      "660/660 [==============================] - 1s 2ms/step - loss: 7.7758 - mean_absolute_error: 7.7758 - val_loss: 8.4114 - val_mean_absolute_error: 8.4114\n",
      "Epoch 103/500\n",
      "660/660 [==============================] - ETA: 0s - loss: 7.7500 - mean_absolute_error: 7.7500\n",
      "Epoch 00103: val_loss did not improve from 7.20456\n",
      "660/660 [==============================] - 2s 2ms/step - loss: 7.7500 - mean_absolute_error: 7.7500 - val_loss: 8.2401 - val_mean_absolute_error: 8.2401\n",
      "Epoch 104/500\n",
      "640/660 [============================>.] - ETA: 0s - loss: 7.6304 - mean_absolute_error: 7.6304\n",
      "Epoch 00104: val_loss did not improve from 7.20456\n",
      "660/660 [==============================] - 1s 2ms/step - loss: 7.6307 - mean_absolute_error: 7.6307 - val_loss: 7.6076 - val_mean_absolute_error: 7.6076\n",
      "Epoch 105/500\n",
      "640/660 [============================>.] - ETA: 0s - loss: 7.8249 - mean_absolute_error: 7.8249\n",
      "Epoch 00105: val_loss did not improve from 7.20456\n",
      "660/660 [==============================] - 1s 2ms/step - loss: 7.8433 - mean_absolute_error: 7.8433 - val_loss: 7.3305 - val_mean_absolute_error: 7.3305\n",
      "Epoch 106/500\n",
      "643/660 [============================>.] - ETA: 0s - loss: 7.7310 - mean_absolute_error: 7.7310\n",
      "Epoch 00106: val_loss did not improve from 7.20456\n",
      "660/660 [==============================] - 1s 2ms/step - loss: 7.7287 - mean_absolute_error: 7.7287 - val_loss: 7.4571 - val_mean_absolute_error: 7.4571\n",
      "Epoch 107/500\n",
      "641/660 [============================>.] - ETA: 0s - loss: 7.6108 - mean_absolute_error: 7.6108\n",
      "Epoch 00107: val_loss did not improve from 7.20456\n",
      "660/660 [==============================] - 1s 2ms/step - loss: 7.6224 - mean_absolute_error: 7.6224 - val_loss: 8.7699 - val_mean_absolute_error: 8.7699\n",
      "Epoch 108/500\n",
      "638/660 [============================>.] - ETA: 0s - loss: 7.7176 - mean_absolute_error: 7.7176\n",
      "Epoch 00108: val_loss did not improve from 7.20456\n",
      "660/660 [==============================] - 1s 2ms/step - loss: 7.7057 - mean_absolute_error: 7.7057 - val_loss: 7.3005 - val_mean_absolute_error: 7.3005\n",
      "Epoch 109/500\n",
      "640/660 [============================>.] - ETA: 0s - loss: 7.7626 - mean_absolute_error: 7.7626\n",
      "Epoch 00109: val_loss did not improve from 7.20456\n",
      "660/660 [==============================] - 1s 2ms/step - loss: 7.7480 - mean_absolute_error: 7.7480 - val_loss: 7.4932 - val_mean_absolute_error: 7.4932\n",
      "Epoch 110/500\n",
      "637/660 [===========================>..] - ETA: 0s - loss: 7.6765 - mean_absolute_error: 7.6765\n",
      "Epoch 00110: val_loss did not improve from 7.20456\n",
      "660/660 [==============================] - 2s 2ms/step - loss: 7.6821 - mean_absolute_error: 7.6821 - val_loss: 7.2051 - val_mean_absolute_error: 7.2051\n",
      "Epoch 111/500\n",
      "656/660 [============================>.] - ETA: 0s - loss: 7.6434 - mean_absolute_error: 7.6434\n",
      "Epoch 00111: val_loss did not improve from 7.20456\n",
      "660/660 [==============================] - 2s 2ms/step - loss: 7.6449 - mean_absolute_error: 7.6449 - val_loss: 7.3225 - val_mean_absolute_error: 7.3225\n",
      "Epoch 112/500\n",
      "641/660 [============================>.] - ETA: 0s - loss: 7.5426 - mean_absolute_error: 7.5426\n",
      "Epoch 00112: val_loss did not improve from 7.20456\n",
      "660/660 [==============================] - 2s 2ms/step - loss: 7.5123 - mean_absolute_error: 7.5123 - val_loss: 7.9174 - val_mean_absolute_error: 7.9174\n",
      "Epoch 113/500\n",
      "645/660 [============================>.] - ETA: 0s - loss: 7.6307 - mean_absolute_error: 7.6307\n",
      "Epoch 00113: val_loss did not improve from 7.20456\n",
      "660/660 [==============================] - 1s 2ms/step - loss: 7.6402 - mean_absolute_error: 7.6402 - val_loss: 7.6236 - val_mean_absolute_error: 7.6236\n",
      "Epoch 114/500\n",
      "646/660 [============================>.] - ETA: 0s - loss: 7.6209 - mean_absolute_error: 7.6209\n",
      "Epoch 00114: val_loss did not improve from 7.20456\n",
      "660/660 [==============================] - 1s 2ms/step - loss: 7.6201 - mean_absolute_error: 7.6201 - val_loss: 7.4758 - val_mean_absolute_error: 7.4758\n",
      "Epoch 115/500\n",
      "648/660 [============================>.] - ETA: 0s - loss: 7.7861 - mean_absolute_error: 7.7861\n",
      "Epoch 00115: val_loss did not improve from 7.20456\n",
      "660/660 [==============================] - 1s 2ms/step - loss: 7.7884 - mean_absolute_error: 7.7884 - val_loss: 7.7516 - val_mean_absolute_error: 7.7516\n",
      "Epoch 116/500\n",
      "649/660 [============================>.] - ETA: 0s - loss: 7.7450 - mean_absolute_error: 7.7450\n",
      "Epoch 00116: val_loss did not improve from 7.20456\n",
      "660/660 [==============================] - 1s 2ms/step - loss: 7.7221 - mean_absolute_error: 7.7221 - val_loss: 7.4072 - val_mean_absolute_error: 7.4072\n",
      "Epoch 117/500\n",
      "649/660 [============================>.] - ETA: 0s - loss: 7.6620 - mean_absolute_error: 7.6620\n",
      "Epoch 00117: val_loss did not improve from 7.20456\n",
      "660/660 [==============================] - 1s 2ms/step - loss: 7.6461 - mean_absolute_error: 7.6461 - val_loss: 7.3873 - val_mean_absolute_error: 7.3873\n",
      "Epoch 118/500\n",
      "648/660 [============================>.] - ETA: 0s - loss: 7.6021 - mean_absolute_error: 7.6021\n",
      "Epoch 00118: val_loss did not improve from 7.20456\n",
      "660/660 [==============================] - 1s 2ms/step - loss: 7.5982 - mean_absolute_error: 7.5982 - val_loss: 7.4680 - val_mean_absolute_error: 7.4680\n",
      "Epoch 119/500\n",
      "650/660 [============================>.] - ETA: 0s - loss: 7.5982 - mean_absolute_error: 7.5982\n",
      "Epoch 00119: val_loss did not improve from 7.20456\n",
      "660/660 [==============================] - 1s 2ms/step - loss: 7.5976 - mean_absolute_error: 7.5976 - val_loss: 7.5359 - val_mean_absolute_error: 7.5359\n",
      "Epoch 120/500\n",
      "648/660 [============================>.] - ETA: 0s - loss: 7.6984 - mean_absolute_error: 7.6984\n",
      "Epoch 00120: val_loss did not improve from 7.20456\n",
      "660/660 [==============================] - 1s 2ms/step - loss: 7.6842 - mean_absolute_error: 7.6842 - val_loss: 7.4599 - val_mean_absolute_error: 7.4599\n",
      "Epoch 121/500\n",
      "649/660 [============================>.] - ETA: 0s - loss: 7.6097 - mean_absolute_error: 7.6097\n",
      "Epoch 00121: val_loss did not improve from 7.20456\n",
      "660/660 [==============================] - 1s 2ms/step - loss: 7.5819 - mean_absolute_error: 7.5819 - val_loss: 7.6502 - val_mean_absolute_error: 7.6502\n",
      "Epoch 122/500\n",
      "651/660 [============================>.] - ETA: 0s - loss: 7.6035 - mean_absolute_error: 7.6035\n",
      "Epoch 00122: val_loss did not improve from 7.20456\n",
      "660/660 [==============================] - 1s 2ms/step - loss: 7.6332 - mean_absolute_error: 7.6332 - val_loss: 7.6584 - val_mean_absolute_error: 7.6584\n",
      "Epoch 123/500\n",
      "650/660 [============================>.] - ETA: 0s - loss: 7.6112 - mean_absolute_error: 7.6112\n",
      "Epoch 00123: val_loss did not improve from 7.20456\n",
      "660/660 [==============================] - 1s 2ms/step - loss: 7.6151 - mean_absolute_error: 7.6151 - val_loss: 7.5256 - val_mean_absolute_error: 7.5256\n",
      "Epoch 124/500\n",
      "649/660 [============================>.] - ETA: 0s - loss: 7.5853 - mean_absolute_error: 7.5853\n",
      "Epoch 00124: val_loss did not improve from 7.20456\n",
      "660/660 [==============================] - 1s 2ms/step - loss: 7.5951 - mean_absolute_error: 7.5951 - val_loss: 7.8874 - val_mean_absolute_error: 7.8874\n",
      "Epoch 125/500\n",
      "650/660 [============================>.] - ETA: 0s - loss: 7.5853 - mean_absolute_error: 7.5853\n",
      "Epoch 00125: val_loss did not improve from 7.20456\n",
      "660/660 [==============================] - 1s 2ms/step - loss: 7.5911 - mean_absolute_error: 7.5911 - val_loss: 7.5851 - val_mean_absolute_error: 7.5851\n",
      "Epoch 126/500\n",
      "645/660 [============================>.] - ETA: 0s - loss: 7.6239 - mean_absolute_error: 7.6239\n",
      "Epoch 00126: val_loss did not improve from 7.20456\n",
      "660/660 [==============================] - 1s 2ms/step - loss: 7.6177 - mean_absolute_error: 7.6177 - val_loss: 7.8272 - val_mean_absolute_error: 7.8272\n",
      "Epoch 127/500\n",
      "647/660 [============================>.] - ETA: 0s - loss: 7.5765 - mean_absolute_error: 7.5765\n",
      "Epoch 00127: val_loss improved from 7.20456 to 7.15565, saving model to Weights-127--7.15565.hdf5\n",
      "660/660 [==============================] - 2s 2ms/step - loss: 7.6030 - mean_absolute_error: 7.6030 - val_loss: 7.1557 - val_mean_absolute_error: 7.1557\n",
      "Epoch 128/500\n",
      "649/660 [============================>.] - ETA: 0s - loss: 7.6936 - mean_absolute_error: 7.6936\n",
      "Epoch 00128: val_loss did not improve from 7.15565\n",
      "660/660 [==============================] - 2s 2ms/step - loss: 7.7016 - mean_absolute_error: 7.7016 - val_loss: 7.8190 - val_mean_absolute_error: 7.8190\n",
      "Epoch 129/500\n",
      "645/660 [============================>.] - ETA: 0s - loss: 7.5729 - mean_absolute_error: 7.5729\n",
      "Epoch 00129: val_loss did not improve from 7.15565\n",
      "660/660 [==============================] - 1s 2ms/step - loss: 7.5839 - mean_absolute_error: 7.5839 - val_loss: 8.9748 - val_mean_absolute_error: 8.9748\n",
      "Epoch 130/500\n",
      "637/660 [===========================>..] - ETA: 0s - loss: 7.5464 - mean_absolute_error: 7.5464\n",
      "Epoch 00130: val_loss did not improve from 7.15565\n",
      "660/660 [==============================] - 2s 2ms/step - loss: 7.5518 - mean_absolute_error: 7.5518 - val_loss: 8.2919 - val_mean_absolute_error: 8.2919\n",
      "Epoch 131/500\n",
      "653/660 [============================>.] - ETA: 0s - loss: 7.6156 - mean_absolute_error: 7.6156\n",
      "Epoch 00131: val_loss did not improve from 7.15565\n",
      "660/660 [==============================] - 2s 2ms/step - loss: 7.6202 - mean_absolute_error: 7.6202 - val_loss: 7.4668 - val_mean_absolute_error: 7.4668\n",
      "Epoch 132/500\n",
      "650/660 [============================>.] - ETA: 0s - loss: 7.5966 - mean_absolute_error: 7.5966\n",
      "Epoch 00132: val_loss improved from 7.15565 to 7.13337, saving model to Weights-132--7.13337.hdf5\n",
      "660/660 [==============================] - 2s 2ms/step - loss: 7.5809 - mean_absolute_error: 7.5809 - val_loss: 7.1334 - val_mean_absolute_error: 7.1334\n",
      "Epoch 133/500\n",
      "650/660 [============================>.] - ETA: 0s - loss: 7.6014 - mean_absolute_error: 7.6014\n",
      "Epoch 00133: val_loss did not improve from 7.13337\n",
      "660/660 [==============================] - 1s 2ms/step - loss: 7.6056 - mean_absolute_error: 7.6056 - val_loss: 7.5333 - val_mean_absolute_error: 7.5333\n",
      "Epoch 134/500\n",
      "651/660 [============================>.] - ETA: 0s - loss: 7.6030 - mean_absolute_error: 7.6030\n",
      "Epoch 00134: val_loss did not improve from 7.13337\n",
      "660/660 [==============================] - 1s 2ms/step - loss: 7.6023 - mean_absolute_error: 7.6023 - val_loss: 7.1685 - val_mean_absolute_error: 7.1685\n",
      "Epoch 135/500\n",
      "648/660 [============================>.] - ETA: 0s - loss: 7.6223 - mean_absolute_error: 7.6223\n",
      "Epoch 00135: val_loss did not improve from 7.13337\n",
      "660/660 [==============================] - 1s 2ms/step - loss: 7.6272 - mean_absolute_error: 7.6272 - val_loss: 7.2684 - val_mean_absolute_error: 7.2684\n",
      "Epoch 136/500\n",
      "648/660 [============================>.] - ETA: 0s - loss: 7.6208 - mean_absolute_error: 7.6208\n",
      "Epoch 00136: val_loss did not improve from 7.13337\n",
      "660/660 [==============================] - 1s 2ms/step - loss: 7.6110 - mean_absolute_error: 7.6110 - val_loss: 7.8559 - val_mean_absolute_error: 7.8559\n",
      "Epoch 137/500\n",
      "646/660 [============================>.] - ETA: 0s - loss: 7.5625 - mean_absolute_error: 7.5625\n",
      "Epoch 00137: val_loss did not improve from 7.13337\n",
      "660/660 [==============================] - 1s 2ms/step - loss: 7.5733 - mean_absolute_error: 7.5733 - val_loss: 7.2771 - val_mean_absolute_error: 7.2771\n",
      "Epoch 138/500\n",
      "645/660 [============================>.] - ETA: 0s - loss: 7.5990 - mean_absolute_error: 7.5990\n",
      "Epoch 00138: val_loss did not improve from 7.13337\n",
      "660/660 [==============================] - 1s 2ms/step - loss: 7.5711 - mean_absolute_error: 7.5711 - val_loss: 7.9582 - val_mean_absolute_error: 7.9582\n",
      "Epoch 139/500\n",
      "648/660 [============================>.] - ETA: 0s - loss: 7.5554 - mean_absolute_error: 7.5554\n",
      "Epoch 00139: val_loss did not improve from 7.13337\n",
      "660/660 [==============================] - 1s 2ms/step - loss: 7.5413 - mean_absolute_error: 7.5413 - val_loss: 7.4805 - val_mean_absolute_error: 7.4805\n",
      "Epoch 140/500\n",
      "649/660 [============================>.] - ETA: 0s - loss: 7.5104 - mean_absolute_error: 7.5104\n",
      "Epoch 00140: val_loss improved from 7.13337 to 7.09586, saving model to Weights-140--7.09586.hdf5\n",
      "660/660 [==============================] - 2s 2ms/step - loss: 7.5082 - mean_absolute_error: 7.5082 - val_loss: 7.0959 - val_mean_absolute_error: 7.0959\n",
      "Epoch 141/500\n",
      "649/660 [============================>.] - ETA: 0s - loss: 7.5543 - mean_absolute_error: 7.5543\n",
      "Epoch 00141: val_loss did not improve from 7.09586\n",
      "660/660 [==============================] - 1s 2ms/step - loss: 7.5784 - mean_absolute_error: 7.5784 - val_loss: 8.2827 - val_mean_absolute_error: 8.2827\n",
      "Epoch 142/500\n",
      "646/660 [============================>.] - ETA: 0s - loss: 7.4739 - mean_absolute_error: 7.4739\n",
      "Epoch 00142: val_loss did not improve from 7.09586\n",
      "660/660 [==============================] - 1s 2ms/step - loss: 7.4685 - mean_absolute_error: 7.4685 - val_loss: 7.3990 - val_mean_absolute_error: 7.3990\n",
      "Epoch 143/500\n",
      "649/660 [============================>.] - ETA: 0s - loss: 7.4937 - mean_absolute_error: 7.4937\n",
      "Epoch 00143: val_loss did not improve from 7.09586\n",
      "660/660 [==============================] - 1s 2ms/step - loss: 7.5003 - mean_absolute_error: 7.5003 - val_loss: 8.2417 - val_mean_absolute_error: 8.2417\n",
      "Epoch 144/500\n",
      "647/660 [============================>.] - ETA: 0s - loss: 7.5129 - mean_absolute_error: 7.5129\n",
      "Epoch 00144: val_loss did not improve from 7.09586\n",
      "660/660 [==============================] - 1s 2ms/step - loss: 7.5279 - mean_absolute_error: 7.5279 - val_loss: 8.1774 - val_mean_absolute_error: 8.1774\n",
      "Epoch 145/500\n",
      "650/660 [============================>.] - ETA: 0s - loss: 7.5604 - mean_absolute_error: 7.5604\n",
      "Epoch 00145: val_loss did not improve from 7.09586\n",
      "660/660 [==============================] - 1s 2ms/step - loss: 7.5523 - mean_absolute_error: 7.5523 - val_loss: 7.1146 - val_mean_absolute_error: 7.1146\n",
      "Epoch 146/500\n",
      "649/660 [============================>.] - ETA: 0s - loss: 7.5426 - mean_absolute_error: 7.5426\n",
      "Epoch 00146: val_loss did not improve from 7.09586\n",
      "660/660 [==============================] - 1s 2ms/step - loss: 7.5519 - mean_absolute_error: 7.5519 - val_loss: 7.4785 - val_mean_absolute_error: 7.4785\n",
      "Epoch 147/500\n",
      "651/660 [============================>.] - ETA: 0s - loss: 7.5681 - mean_absolute_error: 7.5681\n",
      "Epoch 00147: val_loss did not improve from 7.09586\n",
      "660/660 [==============================] - 1s 2ms/step - loss: 7.5628 - mean_absolute_error: 7.5628 - val_loss: 7.8776 - val_mean_absolute_error: 7.8776\n",
      "Epoch 148/500\n",
      "650/660 [============================>.] - ETA: 0s - loss: 7.5642 - mean_absolute_error: 7.5642\n",
      "Epoch 00148: val_loss did not improve from 7.09586\n",
      "660/660 [==============================] - 1s 2ms/step - loss: 7.5680 - mean_absolute_error: 7.5680 - val_loss: 7.4532 - val_mean_absolute_error: 7.4532\n",
      "Epoch 149/500\n",
      "650/660 [============================>.] - ETA: 0s - loss: 7.5504 - mean_absolute_error: 7.5504\n",
      "Epoch 00149: val_loss did not improve from 7.09586\n",
      "660/660 [==============================] - 1s 2ms/step - loss: 7.5334 - mean_absolute_error: 7.5334 - val_loss: 7.2300 - val_mean_absolute_error: 7.2300\n",
      "Epoch 150/500\n",
      "647/660 [============================>.] - ETA: 0s - loss: 7.5540 - mean_absolute_error: 7.5540\n",
      "Epoch 00150: val_loss did not improve from 7.09586\n",
      "660/660 [==============================] - 2s 2ms/step - loss: 7.5606 - mean_absolute_error: 7.5606 - val_loss: 7.9293 - val_mean_absolute_error: 7.9293\n",
      "Epoch 151/500\n",
      "647/660 [============================>.] - ETA: 0s - loss: 7.5870 - mean_absolute_error: 7.5870\n",
      "Epoch 00151: val_loss did not improve from 7.09586\n",
      "660/660 [==============================] - 2s 2ms/step - loss: 7.5888 - mean_absolute_error: 7.5888 - val_loss: 7.2299 - val_mean_absolute_error: 7.2299\n",
      "Epoch 152/500\n",
      "635/660 [===========================>..] - ETA: 0s - loss: 7.5928 - mean_absolute_error: 7.5928\n",
      "Epoch 00152: val_loss did not improve from 7.09586\n",
      "660/660 [==============================] - 1s 2ms/step - loss: 7.6085 - mean_absolute_error: 7.6085 - val_loss: 7.2373 - val_mean_absolute_error: 7.2373\n",
      "Epoch 153/500\n",
      "640/660 [============================>.] - ETA: 0s - loss: 7.5101 - mean_absolute_error: 7.5101\n",
      "Epoch 00153: val_loss did not improve from 7.09586\n",
      "660/660 [==============================] - 1s 2ms/step - loss: 7.5234 - mean_absolute_error: 7.5234 - val_loss: 7.3275 - val_mean_absolute_error: 7.3275\n",
      "Epoch 154/500\n",
      "650/660 [============================>.] - ETA: 0s - loss: 7.4845 - mean_absolute_error: 7.4845\n",
      "Epoch 00154: val_loss improved from 7.09586 to 7.09396, saving model to Weights-154--7.09396.hdf5\n",
      "660/660 [==============================] - 2s 2ms/step - loss: 7.4951 - mean_absolute_error: 7.4951 - val_loss: 7.0940 - val_mean_absolute_error: 7.0940\n",
      "Epoch 155/500\n",
      "642/660 [============================>.] - ETA: 0s - loss: 7.5761 - mean_absolute_error: 7.5761\n",
      "Epoch 00155: val_loss did not improve from 7.09396\n",
      "660/660 [==============================] - 1s 2ms/step - loss: 7.5622 - mean_absolute_error: 7.5622 - val_loss: 7.1039 - val_mean_absolute_error: 7.1039\n",
      "Epoch 156/500\n",
      "651/660 [============================>.] - ETA: 0s - loss: 7.5738 - mean_absolute_error: 7.5738\n",
      "Epoch 00156: val_loss did not improve from 7.09396\n",
      "660/660 [==============================] - 1s 2ms/step - loss: 7.5795 - mean_absolute_error: 7.5795 - val_loss: 7.2853 - val_mean_absolute_error: 7.2853\n",
      "Epoch 157/500\n",
      "653/660 [============================>.] - ETA: 0s - loss: 7.6074 - mean_absolute_error: 7.6074\n",
      "Epoch 00157: val_loss did not improve from 7.09396\n",
      "660/660 [==============================] - 1s 2ms/step - loss: 7.6073 - mean_absolute_error: 7.6073 - val_loss: 7.5520 - val_mean_absolute_error: 7.5520\n",
      "Epoch 158/500\n",
      "644/660 [============================>.] - ETA: 0s - loss: 7.4443 - mean_absolute_error: 7.4443\n",
      "Epoch 00158: val_loss did not improve from 7.09396\n",
      "660/660 [==============================] - 1s 2ms/step - loss: 7.4393 - mean_absolute_error: 7.4393 - val_loss: 7.8169 - val_mean_absolute_error: 7.8169\n",
      "Epoch 159/500\n",
      "649/660 [============================>.] - ETA: 0s - loss: 7.6157 - mean_absolute_error: 7.6157\n",
      "Epoch 00159: val_loss did not improve from 7.09396\n",
      "660/660 [==============================] - 1s 2ms/step - loss: 7.6132 - mean_absolute_error: 7.6132 - val_loss: 8.3898 - val_mean_absolute_error: 8.3898\n",
      "Epoch 160/500\n",
      "639/660 [============================>.] - ETA: 0s - loss: 7.5227 - mean_absolute_error: 7.5227\n",
      "Epoch 00160: val_loss did not improve from 7.09396\n",
      "660/660 [==============================] - 1s 2ms/step - loss: 7.5161 - mean_absolute_error: 7.5161 - val_loss: 7.2045 - val_mean_absolute_error: 7.2045\n",
      "Epoch 161/500\n",
      "641/660 [============================>.] - ETA: 0s - loss: 7.6031 - mean_absolute_error: 7.6031\n",
      "Epoch 00161: val_loss did not improve from 7.09396\n",
      "660/660 [==============================] - 1s 2ms/step - loss: 7.5791 - mean_absolute_error: 7.5791 - val_loss: 8.0294 - val_mean_absolute_error: 8.0294\n",
      "Epoch 162/500\n",
      "660/660 [==============================] - ETA: 0s - loss: 7.4850 - mean_absolute_error: 7.4850\n",
      "Epoch 00162: val_loss did not improve from 7.09396\n",
      "660/660 [==============================] - 1s 2ms/step - loss: 7.4850 - mean_absolute_error: 7.4850 - val_loss: 7.7667 - val_mean_absolute_error: 7.7667\n",
      "Epoch 163/500\n",
      "651/660 [============================>.] - ETA: 0s - loss: 7.5442 - mean_absolute_error: 7.5442\n",
      "Epoch 00163: val_loss did not improve from 7.09396\n",
      "660/660 [==============================] - 1s 2ms/step - loss: 7.5413 - mean_absolute_error: 7.5413 - val_loss: 7.2805 - val_mean_absolute_error: 7.2805\n",
      "Epoch 164/500\n",
      "645/660 [============================>.] - ETA: 0s - loss: 7.5922 - mean_absolute_error: 7.5922\n",
      "Epoch 00164: val_loss did not improve from 7.09396\n",
      "660/660 [==============================] - 1s 2ms/step - loss: 7.5733 - mean_absolute_error: 7.5733 - val_loss: 7.2041 - val_mean_absolute_error: 7.2041\n",
      "Epoch 165/500\n",
      "652/660 [============================>.] - ETA: 0s - loss: 7.4895 - mean_absolute_error: 7.4895\n",
      "Epoch 00165: val_loss did not improve from 7.09396\n",
      "660/660 [==============================] - 1s 2ms/step - loss: 7.4757 - mean_absolute_error: 7.4757 - val_loss: 8.5629 - val_mean_absolute_error: 8.5629\n",
      "Epoch 166/500\n",
      "658/660 [============================>.] - ETA: 0s - loss: 7.5864 - mean_absolute_error: 7.5864\n",
      "Epoch 00166: val_loss improved from 7.09396 to 7.06289, saving model to Weights-166--7.06289.hdf5\n",
      "660/660 [==============================] - 2s 2ms/step - loss: 7.5820 - mean_absolute_error: 7.5820 - val_loss: 7.0629 - val_mean_absolute_error: 7.0629\n",
      "Epoch 167/500\n",
      "655/660 [============================>.] - ETA: 0s - loss: 7.4815 - mean_absolute_error: 7.4815\n",
      "Epoch 00167: val_loss did not improve from 7.06289\n",
      "660/660 [==============================] - 1s 2ms/step - loss: 7.4860 - mean_absolute_error: 7.4860 - val_loss: 7.1701 - val_mean_absolute_error: 7.1701\n",
      "Epoch 168/500\n",
      "653/660 [============================>.] - ETA: 0s - loss: 7.5368 - mean_absolute_error: 7.5368\n",
      "Epoch 00168: val_loss did not improve from 7.06289\n",
      "660/660 [==============================] - 1s 2ms/step - loss: 7.5406 - mean_absolute_error: 7.5406 - val_loss: 7.3782 - val_mean_absolute_error: 7.3782\n",
      "Epoch 169/500\n",
      "657/660 [============================>.] - ETA: 0s - loss: 7.4298 - mean_absolute_error: 7.4298\n",
      "Epoch 00169: val_loss did not improve from 7.06289\n",
      "660/660 [==============================] - 1s 2ms/step - loss: 7.4297 - mean_absolute_error: 7.4297 - val_loss: 7.5034 - val_mean_absolute_error: 7.5034\n",
      "Epoch 170/500\n",
      "655/660 [============================>.] - ETA: 0s - loss: 7.4697 - mean_absolute_error: 7.4697\n",
      "Epoch 00170: val_loss did not improve from 7.06289\n",
      "660/660 [==============================] - 1s 2ms/step - loss: 7.4673 - mean_absolute_error: 7.4673 - val_loss: 7.6191 - val_mean_absolute_error: 7.6191\n",
      "Epoch 171/500\n",
      "654/660 [============================>.] - ETA: 0s - loss: 7.5084 - mean_absolute_error: 7.5084\n",
      "Epoch 00171: val_loss did not improve from 7.06289\n",
      "660/660 [==============================] - 1s 2ms/step - loss: 7.5104 - mean_absolute_error: 7.5104 - val_loss: 7.2914 - val_mean_absolute_error: 7.2914\n",
      "Epoch 172/500\n",
      "639/660 [============================>.] - ETA: 0s - loss: 7.4720 - mean_absolute_error: 7.4720\n",
      "Epoch 00172: val_loss did not improve from 7.06289\n",
      "660/660 [==============================] - 1s 2ms/step - loss: 7.4657 - mean_absolute_error: 7.4657 - val_loss: 7.3742 - val_mean_absolute_error: 7.3742\n",
      "Epoch 173/500\n",
      "660/660 [==============================] - ETA: 0s - loss: 7.5373 - mean_absolute_error: 7.5373\n",
      "Epoch 00173: val_loss did not improve from 7.06289\n",
      "660/660 [==============================] - 1s 2ms/step - loss: 7.5373 - mean_absolute_error: 7.5373 - val_loss: 7.8576 - val_mean_absolute_error: 7.8576\n",
      "Epoch 174/500\n",
      "657/660 [============================>.] - ETA: 0s - loss: 7.4578 - mean_absolute_error: 7.4578\n",
      "Epoch 00174: val_loss did not improve from 7.06289\n",
      "660/660 [==============================] - 1s 2ms/step - loss: 7.4649 - mean_absolute_error: 7.4649 - val_loss: 7.5786 - val_mean_absolute_error: 7.5786\n",
      "Epoch 175/500\n",
      "652/660 [============================>.] - ETA: 0s - loss: 7.5818 - mean_absolute_error: 7.5818\n",
      "Epoch 00175: val_loss did not improve from 7.06289\n",
      "660/660 [==============================] - 1s 2ms/step - loss: 7.5978 - mean_absolute_error: 7.5978 - val_loss: 7.1717 - val_mean_absolute_error: 7.1717\n",
      "Epoch 176/500\n",
      "659/660 [============================>.] - ETA: 0s - loss: 7.5321 - mean_absolute_error: 7.5321\n",
      "Epoch 00176: val_loss did not improve from 7.06289\n",
      "660/660 [==============================] - 1s 2ms/step - loss: 7.5299 - mean_absolute_error: 7.5299 - val_loss: 7.1540 - val_mean_absolute_error: 7.1540\n",
      "Epoch 177/500\n",
      "636/660 [===========================>..] - ETA: 0s - loss: 7.4008 - mean_absolute_error: 7.4008\n",
      "Epoch 00177: val_loss did not improve from 7.06289\n",
      "660/660 [==============================] - 1s 2ms/step - loss: 7.4112 - mean_absolute_error: 7.4112 - val_loss: 7.2219 - val_mean_absolute_error: 7.2219\n",
      "Epoch 178/500\n",
      "652/660 [============================>.] - ETA: 0s - loss: 7.5005 - mean_absolute_error: 7.5005\n",
      "Epoch 00178: val_loss did not improve from 7.06289\n",
      "660/660 [==============================] - 1s 2ms/step - loss: 7.5038 - mean_absolute_error: 7.5038 - val_loss: 7.6977 - val_mean_absolute_error: 7.6977\n",
      "Epoch 179/500\n",
      "647/660 [============================>.] - ETA: 0s - loss: 7.4270 - mean_absolute_error: 7.4270\n",
      "Epoch 00179: val_loss did not improve from 7.06289\n",
      "660/660 [==============================] - 1s 2ms/step - loss: 7.4173 - mean_absolute_error: 7.4173 - val_loss: 7.4742 - val_mean_absolute_error: 7.4742\n",
      "Epoch 180/500\n",
      "648/660 [============================>.] - ETA: 0s - loss: 7.5562 - mean_absolute_error: 7.5562\n",
      "Epoch 00180: val_loss did not improve from 7.06289\n",
      "660/660 [==============================] - 1s 2ms/step - loss: 7.5583 - mean_absolute_error: 7.5583 - val_loss: 7.4055 - val_mean_absolute_error: 7.4055\n",
      "Epoch 181/500\n",
      "651/660 [============================>.] - ETA: 0s - loss: 7.4405 - mean_absolute_error: 7.4405\n",
      "Epoch 00181: val_loss did not improve from 7.06289\n",
      "660/660 [==============================] - 1s 2ms/step - loss: 7.4437 - mean_absolute_error: 7.4437 - val_loss: 7.4797 - val_mean_absolute_error: 7.4797\n",
      "Epoch 182/500\n",
      "651/660 [============================>.] - ETA: 0s - loss: 7.5061 - mean_absolute_error: 7.5061\n",
      "Epoch 00182: val_loss did not improve from 7.06289\n",
      "660/660 [==============================] - 1s 2ms/step - loss: 7.5132 - mean_absolute_error: 7.5132 - val_loss: 7.4308 - val_mean_absolute_error: 7.4308\n",
      "Epoch 183/500\n",
      "648/660 [============================>.] - ETA: 0s - loss: 7.6125 - mean_absolute_error: 7.6125\n",
      "Epoch 00183: val_loss did not improve from 7.06289\n",
      "660/660 [==============================] - 1s 2ms/step - loss: 7.6113 - mean_absolute_error: 7.6113 - val_loss: 7.7463 - val_mean_absolute_error: 7.7463\n",
      "Epoch 184/500\n",
      "656/660 [============================>.] - ETA: 0s - loss: 7.4704 - mean_absolute_error: 7.4704\n",
      "Epoch 00184: val_loss did not improve from 7.06289\n",
      "660/660 [==============================] - 1s 2ms/step - loss: 7.4783 - mean_absolute_error: 7.4783 - val_loss: 7.7909 - val_mean_absolute_error: 7.7909\n",
      "Epoch 185/500\n",
      "642/660 [============================>.] - ETA: 0s - loss: 7.3760 - mean_absolute_error: 7.3760\n",
      "Epoch 00185: val_loss did not improve from 7.06289\n",
      "660/660 [==============================] - 1s 2ms/step - loss: 7.3883 - mean_absolute_error: 7.3883 - val_loss: 7.1561 - val_mean_absolute_error: 7.1561\n",
      "Epoch 186/500\n",
      "653/660 [============================>.] - ETA: 0s - loss: 7.4603 - mean_absolute_error: 7.4603\n",
      "Epoch 00186: val_loss did not improve from 7.06289\n",
      "660/660 [==============================] - 1s 2ms/step - loss: 7.4582 - mean_absolute_error: 7.4582 - val_loss: 7.4591 - val_mean_absolute_error: 7.4591\n",
      "Epoch 187/500\n",
      "653/660 [============================>.] - ETA: 0s - loss: 7.4408 - mean_absolute_error: 7.4408\n",
      "Epoch 00187: val_loss did not improve from 7.06289\n",
      "660/660 [==============================] - 1s 2ms/step - loss: 7.4459 - mean_absolute_error: 7.4459 - val_loss: 7.7775 - val_mean_absolute_error: 7.7775\n",
      "Epoch 188/500\n",
      "655/660 [============================>.] - ETA: 0s - loss: 7.5077 - mean_absolute_error: 7.5077\n",
      "Epoch 00188: val_loss did not improve from 7.06289\n",
      "660/660 [==============================] - 1s 2ms/step - loss: 7.5147 - mean_absolute_error: 7.5147 - val_loss: 7.3253 - val_mean_absolute_error: 7.3253\n",
      "Epoch 189/500\n",
      "659/660 [============================>.] - ETA: 0s - loss: 7.4615 - mean_absolute_error: 7.4615\n",
      "Epoch 00189: val_loss did not improve from 7.06289\n",
      "660/660 [==============================] - 1s 2ms/step - loss: 7.4665 - mean_absolute_error: 7.4665 - val_loss: 7.2460 - val_mean_absolute_error: 7.2460\n",
      "Epoch 190/500\n",
      "638/660 [============================>.] - ETA: 0s - loss: 7.5142 - mean_absolute_error: 7.5142\n",
      "Epoch 00190: val_loss did not improve from 7.06289\n",
      "660/660 [==============================] - 1s 2ms/step - loss: 7.5144 - mean_absolute_error: 7.5144 - val_loss: 8.6914 - val_mean_absolute_error: 8.6914\n",
      "Epoch 191/500\n",
      "643/660 [============================>.] - ETA: 0s - loss: 7.4164 - mean_absolute_error: 7.4164\n",
      "Epoch 00191: val_loss did not improve from 7.06289\n",
      "660/660 [==============================] - 1s 2ms/step - loss: 7.4123 - mean_absolute_error: 7.4123 - val_loss: 7.6087 - val_mean_absolute_error: 7.6087\n",
      "Epoch 192/500\n",
      "659/660 [============================>.] - ETA: 0s - loss: 7.4685 - mean_absolute_error: 7.4685\n",
      "Epoch 00192: val_loss did not improve from 7.06289\n",
      "660/660 [==============================] - 1s 2ms/step - loss: 7.4682 - mean_absolute_error: 7.4682 - val_loss: 7.0914 - val_mean_absolute_error: 7.0914\n",
      "Epoch 193/500\n",
      "655/660 [============================>.] - ETA: 0s - loss: 7.4032 - mean_absolute_error: 7.4032\n",
      "Epoch 00193: val_loss did not improve from 7.06289\n",
      "660/660 [==============================] - 1s 2ms/step - loss: 7.4083 - mean_absolute_error: 7.4083 - val_loss: 7.2305 - val_mean_absolute_error: 7.2305\n",
      "Epoch 194/500\n",
      "637/660 [===========================>..] - ETA: 0s - loss: 7.5438 - mean_absolute_error: 7.5438\n",
      "Epoch 00194: val_loss did not improve from 7.06289\n",
      "660/660 [==============================] - 1s 2ms/step - loss: 7.5366 - mean_absolute_error: 7.5366 - val_loss: 7.1429 - val_mean_absolute_error: 7.1429\n",
      "Epoch 195/500\n",
      "659/660 [============================>.] - ETA: 0s - loss: 7.4966 - mean_absolute_error: 7.4966\n",
      "Epoch 00195: val_loss did not improve from 7.06289\n",
      "660/660 [==============================] - 1s 2ms/step - loss: 7.4951 - mean_absolute_error: 7.4951 - val_loss: 7.3533 - val_mean_absolute_error: 7.3533\n",
      "Epoch 196/500\n",
      "642/660 [============================>.] - ETA: 0s - loss: 7.4310 - mean_absolute_error: 7.4310\n",
      "Epoch 00196: val_loss did not improve from 7.06289\n",
      "660/660 [==============================] - 1s 2ms/step - loss: 7.4360 - mean_absolute_error: 7.4360 - val_loss: 8.0201 - val_mean_absolute_error: 8.0201\n",
      "Epoch 197/500\n",
      "653/660 [============================>.] - ETA: 0s - loss: 7.5588 - mean_absolute_error: 7.5588\n",
      "Epoch 00197: val_loss did not improve from 7.06289\n",
      "660/660 [==============================] - 1s 2ms/step - loss: 7.5526 - mean_absolute_error: 7.5526 - val_loss: 8.1767 - val_mean_absolute_error: 8.1767\n",
      "Epoch 198/500\n",
      "651/660 [============================>.] - ETA: 0s - loss: 7.4538 - mean_absolute_error: 7.4538\n",
      "Epoch 00198: val_loss did not improve from 7.06289\n",
      "660/660 [==============================] - 1s 2ms/step - loss: 7.4658 - mean_absolute_error: 7.4658 - val_loss: 7.9640 - val_mean_absolute_error: 7.9640\n",
      "Epoch 199/500\n",
      "640/660 [============================>.] - ETA: 0s - loss: 7.4453 - mean_absolute_error: 7.4453\n",
      "Epoch 00199: val_loss did not improve from 7.06289\n",
      "660/660 [==============================] - 1s 2ms/step - loss: 7.4524 - mean_absolute_error: 7.4524 - val_loss: 7.2688 - val_mean_absolute_error: 7.2688\n",
      "Epoch 200/500\n",
      "639/660 [============================>.] - ETA: 0s - loss: 7.4356 - mean_absolute_error: 7.4356\n",
      "Epoch 00200: val_loss did not improve from 7.06289\n",
      "660/660 [==============================] - 1s 2ms/step - loss: 7.4400 - mean_absolute_error: 7.4400 - val_loss: 7.1365 - val_mean_absolute_error: 7.1365\n",
      "Epoch 201/500\n",
      "642/660 [============================>.] - ETA: 0s - loss: 7.4664 - mean_absolute_error: 7.4664\n",
      "Epoch 00201: val_loss did not improve from 7.06289\n",
      "660/660 [==============================] - 1s 2ms/step - loss: 7.4676 - mean_absolute_error: 7.4676 - val_loss: 7.2835 - val_mean_absolute_error: 7.2835\n",
      "Epoch 202/500\n",
      "654/660 [============================>.] - ETA: 0s - loss: 7.4303 - mean_absolute_error: 7.4303\n",
      "Epoch 00202: val_loss did not improve from 7.06289\n",
      "660/660 [==============================] - 1s 2ms/step - loss: 7.4314 - mean_absolute_error: 7.4314 - val_loss: 7.3812 - val_mean_absolute_error: 7.3812\n",
      "Epoch 203/500\n",
      "635/660 [===========================>..] - ETA: 0s - loss: 7.5087 - mean_absolute_error: 7.5087\n",
      "Epoch 00203: val_loss did not improve from 7.06289\n",
      "660/660 [==============================] - 1s 2ms/step - loss: 7.5135 - mean_absolute_error: 7.5135 - val_loss: 7.3175 - val_mean_absolute_error: 7.3175\n",
      "Epoch 204/500\n",
      "643/660 [============================>.] - ETA: 0s - loss: 7.4091 - mean_absolute_error: 7.4091\n",
      "Epoch 00204: val_loss did not improve from 7.06289\n",
      "660/660 [==============================] - 1s 2ms/step - loss: 7.4132 - mean_absolute_error: 7.4132 - val_loss: 8.0146 - val_mean_absolute_error: 8.0146\n",
      "Epoch 205/500\n",
      "657/660 [============================>.] - ETA: 0s - loss: 7.4844 - mean_absolute_error: 7.4844\n",
      "Epoch 00205: val_loss did not improve from 7.06289\n",
      "660/660 [==============================] - 1s 2ms/step - loss: 7.4843 - mean_absolute_error: 7.4843 - val_loss: 8.4718 - val_mean_absolute_error: 8.4718\n",
      "Epoch 206/500\n",
      "640/660 [============================>.] - ETA: 0s - loss: 7.4297 - mean_absolute_error: 7.4297\n",
      "Epoch 00206: val_loss did not improve from 7.06289\n",
      "660/660 [==============================] - 1s 2ms/step - loss: 7.4545 - mean_absolute_error: 7.4545 - val_loss: 7.3049 - val_mean_absolute_error: 7.3049\n",
      "Epoch 207/500\n",
      "641/660 [============================>.] - ETA: 0s - loss: 7.4330 - mean_absolute_error: 7.4330\n",
      "Epoch 00207: val_loss did not improve from 7.06289\n",
      "660/660 [==============================] - 1s 2ms/step - loss: 7.4315 - mean_absolute_error: 7.4315 - val_loss: 7.3309 - val_mean_absolute_error: 7.3309\n",
      "Epoch 208/500\n",
      "637/660 [===========================>..] - ETA: 0s - loss: 7.3599 - mean_absolute_error: 7.3599\n",
      "Epoch 00208: val_loss did not improve from 7.06289\n",
      "660/660 [==============================] - 1s 2ms/step - loss: 7.3815 - mean_absolute_error: 7.3815 - val_loss: 9.5910 - val_mean_absolute_error: 9.5910\n",
      "Epoch 209/500\n",
      "645/660 [============================>.] - ETA: 0s - loss: 7.4389 - mean_absolute_error: 7.4389\n",
      "Epoch 00209: val_loss did not improve from 7.06289\n",
      "660/660 [==============================] - 1s 2ms/step - loss: 7.4421 - mean_absolute_error: 7.4421 - val_loss: 7.2587 - val_mean_absolute_error: 7.2587\n",
      "Epoch 210/500\n",
      "636/660 [===========================>..] - ETA: 0s - loss: 7.4765 - mean_absolute_error: 7.4765\n",
      "Epoch 00210: val_loss did not improve from 7.06289\n",
      "660/660 [==============================] - 1s 2ms/step - loss: 7.4623 - mean_absolute_error: 7.4623 - val_loss: 7.3934 - val_mean_absolute_error: 7.3934\n",
      "Epoch 211/500\n",
      "657/660 [============================>.] - ETA: 0s - loss: 7.4017 - mean_absolute_error: 7.4017\n",
      "Epoch 00211: val_loss did not improve from 7.06289\n",
      "660/660 [==============================] - 1s 2ms/step - loss: 7.3999 - mean_absolute_error: 7.3999 - val_loss: 7.9509 - val_mean_absolute_error: 7.9509\n",
      "Epoch 212/500\n",
      "650/660 [============================>.] - ETA: 0s - loss: 7.4798 - mean_absolute_error: 7.4798\n",
      "Epoch 00212: val_loss did not improve from 7.06289\n",
      "660/660 [==============================] - 1s 2ms/step - loss: 7.4800 - mean_absolute_error: 7.4800 - val_loss: 8.2463 - val_mean_absolute_error: 8.2463\n",
      "Epoch 213/500\n",
      "657/660 [============================>.] - ETA: 0s - loss: 7.4168 - mean_absolute_error: 7.4168\n",
      "Epoch 00213: val_loss did not improve from 7.06289\n",
      "660/660 [==============================] - 1s 2ms/step - loss: 7.4250 - mean_absolute_error: 7.4250 - val_loss: 7.4965 - val_mean_absolute_error: 7.4965\n",
      "Epoch 214/500\n",
      "642/660 [============================>.] - ETA: 0s - loss: 7.5371 - mean_absolute_error: 7.5371\n",
      "Epoch 00214: val_loss did not improve from 7.06289\n",
      "660/660 [==============================] - 1s 2ms/step - loss: 7.5220 - mean_absolute_error: 7.5220 - val_loss: 7.1858 - val_mean_absolute_error: 7.1858\n",
      "Epoch 215/500\n",
      "652/660 [============================>.] - ETA: 0s - loss: 7.4651 - mean_absolute_error: 7.4651\n",
      "Epoch 00215: val_loss did not improve from 7.06289\n",
      "660/660 [==============================] - 1s 2ms/step - loss: 7.4677 - mean_absolute_error: 7.4677 - val_loss: 7.2079 - val_mean_absolute_error: 7.2079\n",
      "Epoch 216/500\n",
      "646/660 [============================>.] - ETA: 0s - loss: 7.3882 - mean_absolute_error: 7.3882\n",
      "Epoch 00216: val_loss did not improve from 7.06289\n",
      "660/660 [==============================] - 1s 2ms/step - loss: 7.3800 - mean_absolute_error: 7.3800 - val_loss: 7.5523 - val_mean_absolute_error: 7.5523\n",
      "Epoch 217/500\n",
      "648/660 [============================>.] - ETA: 0s - loss: 7.4263 - mean_absolute_error: 7.4263\n",
      "Epoch 00217: val_loss did not improve from 7.06289\n",
      "660/660 [==============================] - 1s 2ms/step - loss: 7.4096 - mean_absolute_error: 7.4096 - val_loss: 7.5897 - val_mean_absolute_error: 7.5897\n",
      "Epoch 218/500\n",
      "650/660 [============================>.] - ETA: 0s - loss: 7.4857 - mean_absolute_error: 7.4857\n",
      "Epoch 00218: val_loss did not improve from 7.06289\n",
      "660/660 [==============================] - 1s 2ms/step - loss: 7.4949 - mean_absolute_error: 7.4949 - val_loss: 7.3282 - val_mean_absolute_error: 7.3282\n",
      "Epoch 219/500\n",
      "644/660 [============================>.] - ETA: 0s - loss: 7.3823 - mean_absolute_error: 7.3823\n",
      "Epoch 00219: val_loss did not improve from 7.06289\n",
      "660/660 [==============================] - 1s 2ms/step - loss: 7.3734 - mean_absolute_error: 7.3734 - val_loss: 7.5712 - val_mean_absolute_error: 7.5712\n",
      "Epoch 220/500\n",
      "660/660 [==============================] - ETA: 0s - loss: 7.4436 - mean_absolute_error: 7.4436\n",
      "Epoch 00220: val_loss did not improve from 7.06289\n",
      "660/660 [==============================] - 1s 2ms/step - loss: 7.4436 - mean_absolute_error: 7.4436 - val_loss: 8.4442 - val_mean_absolute_error: 8.4442\n",
      "Epoch 221/500\n",
      "636/660 [===========================>..] - ETA: 0s - loss: 7.3551 - mean_absolute_error: 7.3551\n",
      "Epoch 00221: val_loss did not improve from 7.06289\n",
      "660/660 [==============================] - 1s 2ms/step - loss: 7.3699 - mean_absolute_error: 7.3699 - val_loss: 7.5328 - val_mean_absolute_error: 7.5328\n",
      "Epoch 222/500\n",
      "648/660 [============================>.] - ETA: 0s - loss: 7.4267 - mean_absolute_error: 7.4267\n",
      "Epoch 00222: val_loss did not improve from 7.06289\n",
      "660/660 [==============================] - 1s 2ms/step - loss: 7.4102 - mean_absolute_error: 7.4102 - val_loss: 8.1851 - val_mean_absolute_error: 8.1851\n",
      "Epoch 223/500\n",
      "651/660 [============================>.] - ETA: 0s - loss: 7.5046 - mean_absolute_error: 7.5046\n",
      "Epoch 00223: val_loss did not improve from 7.06289\n",
      "660/660 [==============================] - 1s 2ms/step - loss: 7.5083 - mean_absolute_error: 7.5083 - val_loss: 7.5365 - val_mean_absolute_error: 7.5365\n",
      "Epoch 224/500\n",
      "642/660 [============================>.] - ETA: 0s - loss: 7.3949 - mean_absolute_error: 7.3949\n",
      "Epoch 00224: val_loss did not improve from 7.06289\n",
      "660/660 [==============================] - 1s 2ms/step - loss: 7.4075 - mean_absolute_error: 7.4075 - val_loss: 7.1425 - val_mean_absolute_error: 7.1425\n",
      "Epoch 225/500\n",
      "653/660 [============================>.] - ETA: 0s - loss: 7.4069 - mean_absolute_error: 7.4069\n",
      "Epoch 00225: val_loss did not improve from 7.06289\n",
      "660/660 [==============================] - 1s 2ms/step - loss: 7.4148 - mean_absolute_error: 7.4148 - val_loss: 8.1793 - val_mean_absolute_error: 8.1793\n",
      "Epoch 226/500\n",
      "641/660 [============================>.] - ETA: 0s - loss: 7.3826 - mean_absolute_error: 7.3826\n",
      "Epoch 00226: val_loss did not improve from 7.06289\n",
      "660/660 [==============================] - 1s 2ms/step - loss: 7.3823 - mean_absolute_error: 7.3823 - val_loss: 7.2272 - val_mean_absolute_error: 7.2272\n",
      "Epoch 227/500\n",
      "645/660 [============================>.] - ETA: 0s - loss: 7.4382 - mean_absolute_error: 7.4382\n",
      "Epoch 00227: val_loss did not improve from 7.06289\n",
      "660/660 [==============================] - 1s 2ms/step - loss: 7.4358 - mean_absolute_error: 7.4358 - val_loss: 7.2002 - val_mean_absolute_error: 7.2002\n",
      "Epoch 228/500\n",
      "643/660 [============================>.] - ETA: 0s - loss: 7.4085 - mean_absolute_error: 7.4085\n",
      "Epoch 00228: val_loss improved from 7.06289 to 7.05397, saving model to Weights-228--7.05397.hdf5\n",
      "660/660 [==============================] - 2s 2ms/step - loss: 7.4158 - mean_absolute_error: 7.4158 - val_loss: 7.0540 - val_mean_absolute_error: 7.0540\n",
      "Epoch 229/500\n",
      "644/660 [============================>.] - ETA: 0s - loss: 7.3865 - mean_absolute_error: 7.3865\n",
      "Epoch 00229: val_loss did not improve from 7.05397\n",
      "660/660 [==============================] - 1s 2ms/step - loss: 7.3791 - mean_absolute_error: 7.3791 - val_loss: 7.1665 - val_mean_absolute_error: 7.1665\n",
      "Epoch 230/500\n",
      "640/660 [============================>.] - ETA: 0s - loss: 7.3532 - mean_absolute_error: 7.3532\n",
      "Epoch 00230: val_loss did not improve from 7.05397\n",
      "660/660 [==============================] - 1s 2ms/step - loss: 7.3577 - mean_absolute_error: 7.3577 - val_loss: 7.6985 - val_mean_absolute_error: 7.6985\n",
      "Epoch 231/500\n",
      "635/660 [===========================>..] - ETA: 0s - loss: 7.3701 - mean_absolute_error: 7.3701\n",
      "Epoch 00231: val_loss did not improve from 7.05397\n",
      "660/660 [==============================] - 1s 2ms/step - loss: 7.3810 - mean_absolute_error: 7.3810 - val_loss: 7.5672 - val_mean_absolute_error: 7.5672\n",
      "Epoch 232/500\n",
      "639/660 [============================>.] - ETA: 0s - loss: 7.4085 - mean_absolute_error: 7.4085\n",
      "Epoch 00232: val_loss did not improve from 7.05397\n",
      "660/660 [==============================] - 1s 2ms/step - loss: 7.4210 - mean_absolute_error: 7.4210 - val_loss: 7.1583 - val_mean_absolute_error: 7.1583\n",
      "Epoch 233/500\n",
      "636/660 [===========================>..] - ETA: 0s - loss: 7.4760 - mean_absolute_error: 7.4760\n",
      "Epoch 00233: val_loss did not improve from 7.05397\n",
      "660/660 [==============================] - 1s 2ms/step - loss: 7.4596 - mean_absolute_error: 7.4596 - val_loss: 7.7728 - val_mean_absolute_error: 7.7728\n",
      "Epoch 234/500\n",
      "637/660 [===========================>..] - ETA: 0s - loss: 7.4183 - mean_absolute_error: 7.4183\n",
      "Epoch 00234: val_loss did not improve from 7.05397\n",
      "660/660 [==============================] - 1s 2ms/step - loss: 7.4244 - mean_absolute_error: 7.4244 - val_loss: 7.0771 - val_mean_absolute_error: 7.0771\n",
      "Epoch 235/500\n",
      "640/660 [============================>.] - ETA: 0s - loss: 7.4447 - mean_absolute_error: 7.4447\n",
      "Epoch 00235: val_loss did not improve from 7.05397\n",
      "660/660 [==============================] - 1s 2ms/step - loss: 7.4335 - mean_absolute_error: 7.4335 - val_loss: 7.1656 - val_mean_absolute_error: 7.1656\n",
      "Epoch 236/500\n",
      "653/660 [============================>.] - ETA: 0s - loss: 7.4573 - mean_absolute_error: 7.4573\n",
      "Epoch 00236: val_loss did not improve from 7.05397\n",
      "660/660 [==============================] - 1s 2ms/step - loss: 7.4533 - mean_absolute_error: 7.4533 - val_loss: 7.1419 - val_mean_absolute_error: 7.1419\n",
      "Epoch 237/500\n",
      "652/660 [============================>.] - ETA: 0s - loss: 7.3462 - mean_absolute_error: 7.3462\n",
      "Epoch 00237: val_loss did not improve from 7.05397\n",
      "660/660 [==============================] - 1s 2ms/step - loss: 7.3474 - mean_absolute_error: 7.3474 - val_loss: 7.1566 - val_mean_absolute_error: 7.1566\n",
      "Epoch 238/500\n",
      "654/660 [============================>.] - ETA: 0s - loss: 7.3280 - mean_absolute_error: 7.3280\n",
      "Epoch 00238: val_loss did not improve from 7.05397\n",
      "660/660 [==============================] - 1s 2ms/step - loss: 7.3328 - mean_absolute_error: 7.3328 - val_loss: 7.1448 - val_mean_absolute_error: 7.1448\n",
      "Epoch 239/500\n",
      "636/660 [===========================>..] - ETA: 0s - loss: 7.4134 - mean_absolute_error: 7.4134\n",
      "Epoch 00239: val_loss did not improve from 7.05397\n",
      "660/660 [==============================] - 1s 2ms/step - loss: 7.3760 - mean_absolute_error: 7.3760 - val_loss: 7.1554 - val_mean_absolute_error: 7.1554\n",
      "Epoch 240/500\n",
      "658/660 [============================>.] - ETA: 0s - loss: 7.3924 - mean_absolute_error: 7.3924\n",
      "Epoch 00240: val_loss did not improve from 7.05397\n",
      "660/660 [==============================] - 1s 2ms/step - loss: 7.3930 - mean_absolute_error: 7.3930 - val_loss: 7.0898 - val_mean_absolute_error: 7.0898\n",
      "Epoch 241/500\n",
      "659/660 [============================>.] - ETA: 0s - loss: 7.4335 - mean_absolute_error: 7.4335\n",
      "Epoch 00241: val_loss improved from 7.05397 to 7.05377, saving model to Weights-241--7.05377.hdf5\n",
      "660/660 [==============================] - 2s 2ms/step - loss: 7.4336 - mean_absolute_error: 7.4336 - val_loss: 7.0538 - val_mean_absolute_error: 7.0538\n",
      "Epoch 242/500\n",
      "640/660 [============================>.] - ETA: 0s - loss: 7.4589 - mean_absolute_error: 7.4589\n",
      "Epoch 00242: val_loss did not improve from 7.05377\n",
      "660/660 [==============================] - 1s 2ms/step - loss: 7.4531 - mean_absolute_error: 7.4531 - val_loss: 7.1440 - val_mean_absolute_error: 7.1440\n",
      "Epoch 243/500\n",
      "648/660 [============================>.] - ETA: 0s - loss: 7.3856 - mean_absolute_error: 7.3856\n",
      "Epoch 00243: val_loss did not improve from 7.05377\n",
      "660/660 [==============================] - 1s 2ms/step - loss: 7.3819 - mean_absolute_error: 7.3819 - val_loss: 7.4927 - val_mean_absolute_error: 7.4927\n",
      "Epoch 244/500\n",
      "659/660 [============================>.] - ETA: 0s - loss: 7.3713 - mean_absolute_error: 7.3713\n",
      "Epoch 00244: val_loss did not improve from 7.05377\n",
      "660/660 [==============================] - 1s 2ms/step - loss: 7.3720 - mean_absolute_error: 7.3720 - val_loss: 8.6251 - val_mean_absolute_error: 8.6251\n",
      "Epoch 245/500\n",
      "636/660 [===========================>..] - ETA: 0s - loss: 7.4268 - mean_absolute_error: 7.4268\n",
      "Epoch 00245: val_loss did not improve from 7.05377\n",
      "660/660 [==============================] - 1s 2ms/step - loss: 7.4280 - mean_absolute_error: 7.4280 - val_loss: 7.1099 - val_mean_absolute_error: 7.1099\n",
      "Epoch 246/500\n",
      "650/660 [============================>.] - ETA: 0s - loss: 7.4040 - mean_absolute_error: 7.4040\n",
      "Epoch 00246: val_loss did not improve from 7.05377\n",
      "660/660 [==============================] - 1s 2ms/step - loss: 7.4046 - mean_absolute_error: 7.4046 - val_loss: 7.1794 - val_mean_absolute_error: 7.1794\n",
      "Epoch 247/500\n",
      "648/660 [============================>.] - ETA: 0s - loss: 7.3990 - mean_absolute_error: 7.3990\n",
      "Epoch 00247: val_loss did not improve from 7.05377\n",
      "660/660 [==============================] - 1s 2ms/step - loss: 7.3819 - mean_absolute_error: 7.3819 - val_loss: 7.3048 - val_mean_absolute_error: 7.3048\n",
      "Epoch 248/500\n",
      "651/660 [============================>.] - ETA: 0s - loss: 7.3744 - mean_absolute_error: 7.3744\n",
      "Epoch 00248: val_loss did not improve from 7.05377\n",
      "660/660 [==============================] - 1s 2ms/step - loss: 7.3709 - mean_absolute_error: 7.3709 - val_loss: 7.3819 - val_mean_absolute_error: 7.3819\n",
      "Epoch 249/500\n",
      "645/660 [============================>.] - ETA: 0s - loss: 7.3273 - mean_absolute_error: 7.3273\n",
      "Epoch 00249: val_loss did not improve from 7.05377\n",
      "660/660 [==============================] - 1s 2ms/step - loss: 7.3196 - mean_absolute_error: 7.3196 - val_loss: 8.3119 - val_mean_absolute_error: 8.3119\n",
      "Epoch 250/500\n",
      "656/660 [============================>.] - ETA: 0s - loss: 7.4096 - mean_absolute_error: 7.4096\n",
      "Epoch 00250: val_loss did not improve from 7.05377\n",
      "660/660 [==============================] - 1s 2ms/step - loss: 7.4088 - mean_absolute_error: 7.4088 - val_loss: 7.1174 - val_mean_absolute_error: 7.1174\n",
      "Epoch 251/500\n",
      "660/660 [==============================] - ETA: 0s - loss: 7.3618 - mean_absolute_error: 7.3618\n",
      "Epoch 00251: val_loss did not improve from 7.05377\n",
      "660/660 [==============================] - 1s 2ms/step - loss: 7.3618 - mean_absolute_error: 7.3618 - val_loss: 7.4983 - val_mean_absolute_error: 7.4983\n",
      "Epoch 252/500\n",
      "645/660 [============================>.] - ETA: 0s - loss: 7.3857 - mean_absolute_error: 7.3857\n",
      "Epoch 00252: val_loss did not improve from 7.05377\n",
      "660/660 [==============================] - 1s 2ms/step - loss: 7.3989 - mean_absolute_error: 7.3989 - val_loss: 7.7369 - val_mean_absolute_error: 7.7369\n",
      "Epoch 253/500\n",
      "656/660 [============================>.] - ETA: 0s - loss: 7.3946 - mean_absolute_error: 7.3946\n",
      "Epoch 00253: val_loss did not improve from 7.05377\n",
      "660/660 [==============================] - 1s 2ms/step - loss: 7.3903 - mean_absolute_error: 7.3903 - val_loss: 7.1534 - val_mean_absolute_error: 7.1534\n",
      "Epoch 254/500\n",
      "655/660 [============================>.] - ETA: 0s - loss: 7.3696 - mean_absolute_error: 7.3696\n",
      "Epoch 00254: val_loss did not improve from 7.05377\n",
      "660/660 [==============================] - 1s 2ms/step - loss: 7.3751 - mean_absolute_error: 7.3751 - val_loss: 7.2654 - val_mean_absolute_error: 7.2654\n",
      "Epoch 255/500\n",
      "653/660 [============================>.] - ETA: 0s - loss: 7.4612 - mean_absolute_error: 7.4612\n",
      "Epoch 00255: val_loss did not improve from 7.05377\n",
      "660/660 [==============================] - 1s 2ms/step - loss: 7.4602 - mean_absolute_error: 7.4602 - val_loss: 7.1919 - val_mean_absolute_error: 7.1919\n",
      "Epoch 256/500\n",
      "659/660 [============================>.] - ETA: 0s - loss: 7.3821 - mean_absolute_error: 7.3821\n",
      "Epoch 00256: val_loss did not improve from 7.05377\n",
      "660/660 [==============================] - 1s 2ms/step - loss: 7.3825 - mean_absolute_error: 7.3825 - val_loss: 7.8817 - val_mean_absolute_error: 7.8817\n",
      "Epoch 257/500\n",
      "656/660 [============================>.] - ETA: 0s - loss: 7.4024 - mean_absolute_error: 7.4024\n",
      "Epoch 00257: val_loss did not improve from 7.05377\n",
      "660/660 [==============================] - 1s 2ms/step - loss: 7.4004 - mean_absolute_error: 7.4004 - val_loss: 8.0997 - val_mean_absolute_error: 8.0997\n",
      "Epoch 258/500\n",
      "647/660 [============================>.] - ETA: 0s - loss: 7.4372 - mean_absolute_error: 7.4372\n",
      "Epoch 00258: val_loss did not improve from 7.05377\n",
      "660/660 [==============================] - 1s 2ms/step - loss: 7.4348 - mean_absolute_error: 7.4348 - val_loss: 7.0547 - val_mean_absolute_error: 7.0547\n",
      "Epoch 259/500\n",
      "645/660 [============================>.] - ETA: 0s - loss: 7.3163 - mean_absolute_error: 7.3163\n",
      "Epoch 00259: val_loss did not improve from 7.05377\n",
      "660/660 [==============================] - 1s 2ms/step - loss: 7.3146 - mean_absolute_error: 7.3146 - val_loss: 7.2244 - val_mean_absolute_error: 7.2244\n",
      "Epoch 260/500\n",
      "653/660 [============================>.] - ETA: 0s - loss: 7.4179 - mean_absolute_error: 7.4179\n",
      "Epoch 00260: val_loss did not improve from 7.05377\n",
      "660/660 [==============================] - 1s 2ms/step - loss: 7.4128 - mean_absolute_error: 7.4128 - val_loss: 7.1696 - val_mean_absolute_error: 7.1696\n",
      "Epoch 261/500\n",
      "642/660 [============================>.] - ETA: 0s - loss: 7.3384 - mean_absolute_error: 7.3384\n",
      "Epoch 00261: val_loss did not improve from 7.05377\n",
      "660/660 [==============================] - 1s 2ms/step - loss: 7.3426 - mean_absolute_error: 7.3426 - val_loss: 7.3196 - val_mean_absolute_error: 7.3196\n",
      "Epoch 262/500\n",
      "658/660 [============================>.] - ETA: 0s - loss: 7.3996 - mean_absolute_error: 7.3996\n",
      "Epoch 00262: val_loss did not improve from 7.05377\n",
      "660/660 [==============================] - 1s 2ms/step - loss: 7.4011 - mean_absolute_error: 7.4011 - val_loss: 7.2825 - val_mean_absolute_error: 7.2825\n",
      "Epoch 263/500\n",
      "660/660 [==============================] - ETA: 0s - loss: 7.3777 - mean_absolute_error: 7.3777\n",
      "Epoch 00263: val_loss did not improve from 7.05377\n",
      "660/660 [==============================] - 1s 2ms/step - loss: 7.3777 - mean_absolute_error: 7.3777 - val_loss: 7.6984 - val_mean_absolute_error: 7.6984\n",
      "Epoch 264/500\n",
      "646/660 [============================>.] - ETA: 0s - loss: 7.4019 - mean_absolute_error: 7.4019\n",
      "Epoch 00264: val_loss did not improve from 7.05377\n",
      "660/660 [==============================] - 1s 2ms/step - loss: 7.4049 - mean_absolute_error: 7.4049 - val_loss: 7.2619 - val_mean_absolute_error: 7.2619\n",
      "Epoch 265/500\n",
      "658/660 [============================>.] - ETA: 0s - loss: 7.4258 - mean_absolute_error: 7.4258\n",
      "Epoch 00265: val_loss did not improve from 7.05377\n",
      "660/660 [==============================] - 1s 2ms/step - loss: 7.4330 - mean_absolute_error: 7.4330 - val_loss: 7.2880 - val_mean_absolute_error: 7.2880\n",
      "Epoch 266/500\n",
      "638/660 [============================>.] - ETA: 0s - loss: 7.3250 - mean_absolute_error: 7.3250\n",
      "Epoch 00266: val_loss did not improve from 7.05377\n",
      "660/660 [==============================] - 1s 2ms/step - loss: 7.3160 - mean_absolute_error: 7.3160 - val_loss: 7.1420 - val_mean_absolute_error: 7.1420\n",
      "Epoch 267/500\n",
      "641/660 [============================>.] - ETA: 0s - loss: 7.2779 - mean_absolute_error: 7.2779\n",
      "Epoch 00267: val_loss did not improve from 7.05377\n",
      "660/660 [==============================] - 1s 2ms/step - loss: 7.2665 - mean_absolute_error: 7.2665 - val_loss: 7.1093 - val_mean_absolute_error: 7.1093\n",
      "Epoch 268/500\n",
      "644/660 [============================>.] - ETA: 0s - loss: 7.4226 - mean_absolute_error: 7.4226\n",
      "Epoch 00268: val_loss did not improve from 7.05377\n",
      "660/660 [==============================] - 1s 2ms/step - loss: 7.4128 - mean_absolute_error: 7.4128 - val_loss: 7.2498 - val_mean_absolute_error: 7.2498\n",
      "Epoch 269/500\n",
      "650/660 [============================>.] - ETA: 0s - loss: 7.2898 - mean_absolute_error: 7.2898\n",
      "Epoch 00269: val_loss did not improve from 7.05377\n",
      "660/660 [==============================] - 1s 2ms/step - loss: 7.2843 - mean_absolute_error: 7.2843 - val_loss: 7.2896 - val_mean_absolute_error: 7.2896\n",
      "Epoch 270/500\n",
      "658/660 [============================>.] - ETA: 0s - loss: 7.3612 - mean_absolute_error: 7.3612\n",
      "Epoch 00270: val_loss did not improve from 7.05377\n",
      "660/660 [==============================] - 1s 2ms/step - loss: 7.3623 - mean_absolute_error: 7.3623 - val_loss: 7.6202 - val_mean_absolute_error: 7.6202\n",
      "Epoch 271/500\n",
      "651/660 [============================>.] - ETA: 0s - loss: 7.3703 - mean_absolute_error: 7.3703\n",
      "Epoch 00271: val_loss did not improve from 7.05377\n",
      "660/660 [==============================] - 1s 2ms/step - loss: 7.3923 - mean_absolute_error: 7.3923 - val_loss: 7.1219 - val_mean_absolute_error: 7.1219\n",
      "Epoch 272/500\n",
      "650/660 [============================>.] - ETA: 0s - loss: 7.4206 - mean_absolute_error: 7.4206\n",
      "Epoch 00272: val_loss did not improve from 7.05377\n",
      "660/660 [==============================] - 1s 2ms/step - loss: 7.4234 - mean_absolute_error: 7.4234 - val_loss: 7.2664 - val_mean_absolute_error: 7.2664\n",
      "Epoch 273/500\n",
      "652/660 [============================>.] - ETA: 0s - loss: 7.4724 - mean_absolute_error: 7.4724\n",
      "Epoch 00273: val_loss did not improve from 7.05377\n",
      "660/660 [==============================] - 1s 2ms/step - loss: 7.4512 - mean_absolute_error: 7.4512 - val_loss: 7.2244 - val_mean_absolute_error: 7.2244\n",
      "Epoch 274/500\n",
      "650/660 [============================>.] - ETA: 0s - loss: 7.4011 - mean_absolute_error: 7.4011\n",
      "Epoch 00274: val_loss did not improve from 7.05377\n",
      "660/660 [==============================] - 1s 2ms/step - loss: 7.4057 - mean_absolute_error: 7.4057 - val_loss: 7.8592 - val_mean_absolute_error: 7.8592\n",
      "Epoch 275/500\n",
      "641/660 [============================>.] - ETA: 0s - loss: 7.3370 - mean_absolute_error: 7.3370\n",
      "Epoch 00275: val_loss did not improve from 7.05377\n",
      "660/660 [==============================] - 1s 2ms/step - loss: 7.3468 - mean_absolute_error: 7.3468 - val_loss: 7.0919 - val_mean_absolute_error: 7.0919\n",
      "Epoch 276/500\n",
      "655/660 [============================>.] - ETA: 0s - loss: 7.3177 - mean_absolute_error: 7.3177\n",
      "Epoch 00276: val_loss did not improve from 7.05377\n",
      "660/660 [==============================] - 1s 2ms/step - loss: 7.3216 - mean_absolute_error: 7.3216 - val_loss: 7.1208 - val_mean_absolute_error: 7.1208\n",
      "Epoch 277/500\n",
      "639/660 [============================>.] - ETA: 0s - loss: 7.4129 - mean_absolute_error: 7.4129\n",
      "Epoch 00277: val_loss improved from 7.05377 to 7.04707, saving model to Weights-277--7.04707.hdf5\n",
      "660/660 [==============================] - 2s 2ms/step - loss: 7.4122 - mean_absolute_error: 7.4122 - val_loss: 7.0471 - val_mean_absolute_error: 7.0471\n",
      "Epoch 278/500\n",
      "659/660 [============================>.] - ETA: 0s - loss: 7.3764 - mean_absolute_error: 7.3764\n",
      "Epoch 00278: val_loss did not improve from 7.04707\n",
      "660/660 [==============================] - 1s 2ms/step - loss: 7.3731 - mean_absolute_error: 7.3731 - val_loss: 7.2167 - val_mean_absolute_error: 7.2167\n",
      "Epoch 279/500\n",
      "659/660 [============================>.] - ETA: 0s - loss: 7.4188 - mean_absolute_error: 7.4188\n",
      "Epoch 00279: val_loss did not improve from 7.04707\n",
      "660/660 [==============================] - 1s 2ms/step - loss: 7.4172 - mean_absolute_error: 7.4172 - val_loss: 7.6101 - val_mean_absolute_error: 7.6101\n",
      "Epoch 280/500\n",
      "651/660 [============================>.] - ETA: 0s - loss: 7.3182 - mean_absolute_error: 7.3182\n",
      "Epoch 00280: val_loss did not improve from 7.04707\n",
      "660/660 [==============================] - 1s 2ms/step - loss: 7.3215 - mean_absolute_error: 7.3215 - val_loss: 7.6255 - val_mean_absolute_error: 7.6255\n",
      "Epoch 281/500\n",
      "637/660 [===========================>..] - ETA: 0s - loss: 7.3354 - mean_absolute_error: 7.3354\n",
      "Epoch 00281: val_loss did not improve from 7.04707\n",
      "660/660 [==============================] - 1s 2ms/step - loss: 7.3483 - mean_absolute_error: 7.3483 - val_loss: 7.1880 - val_mean_absolute_error: 7.1880\n",
      "Epoch 282/500\n",
      "658/660 [============================>.] - ETA: 0s - loss: 7.3587 - mean_absolute_error: 7.3587\n",
      "Epoch 00282: val_loss did not improve from 7.04707\n",
      "660/660 [==============================] - 1s 2ms/step - loss: 7.3525 - mean_absolute_error: 7.3525 - val_loss: 7.2333 - val_mean_absolute_error: 7.2333\n",
      "Epoch 283/500\n",
      "648/660 [============================>.] - ETA: 0s - loss: 7.3641 - mean_absolute_error: 7.3641\n",
      "Epoch 00283: val_loss did not improve from 7.04707\n",
      "660/660 [==============================] - 1s 2ms/step - loss: 7.3711 - mean_absolute_error: 7.3711 - val_loss: 8.7301 - val_mean_absolute_error: 8.7301\n",
      "Epoch 284/500\n",
      "639/660 [============================>.] - ETA: 0s - loss: 7.3719 - mean_absolute_error: 7.3719\n",
      "Epoch 00284: val_loss did not improve from 7.04707\n",
      "660/660 [==============================] - 1s 2ms/step - loss: 7.3711 - mean_absolute_error: 7.3711 - val_loss: 7.4122 - val_mean_absolute_error: 7.4122\n",
      "Epoch 285/500\n",
      "656/660 [============================>.] - ETA: 0s - loss: 7.4076 - mean_absolute_error: 7.4076\n",
      "Epoch 00285: val_loss did not improve from 7.04707\n",
      "660/660 [==============================] - 1s 2ms/step - loss: 7.3985 - mean_absolute_error: 7.3985 - val_loss: 7.0870 - val_mean_absolute_error: 7.0870\n",
      "Epoch 286/500\n",
      "636/660 [===========================>..] - ETA: 0s - loss: 7.3580 - mean_absolute_error: 7.3580\n",
      "Epoch 00286: val_loss did not improve from 7.04707\n",
      "660/660 [==============================] - 1s 2ms/step - loss: 7.3555 - mean_absolute_error: 7.3555 - val_loss: 7.3740 - val_mean_absolute_error: 7.3740\n",
      "Epoch 287/500\n",
      "659/660 [============================>.] - ETA: 0s - loss: 7.3499 - mean_absolute_error: 7.3499\n",
      "Epoch 00287: val_loss did not improve from 7.04707\n",
      "660/660 [==============================] - 1s 2ms/step - loss: 7.3482 - mean_absolute_error: 7.3482 - val_loss: 7.2815 - val_mean_absolute_error: 7.2815\n",
      "Epoch 288/500\n",
      "637/660 [===========================>..] - ETA: 0s - loss: 7.3288 - mean_absolute_error: 7.3288\n",
      "Epoch 00288: val_loss did not improve from 7.04707\n",
      "660/660 [==============================] - 1s 2ms/step - loss: 7.3275 - mean_absolute_error: 7.3275 - val_loss: 7.2729 - val_mean_absolute_error: 7.2729\n",
      "Epoch 289/500\n",
      "640/660 [============================>.] - ETA: 0s - loss: 7.3484 - mean_absolute_error: 7.3484\n",
      "Epoch 00289: val_loss did not improve from 7.04707\n",
      "660/660 [==============================] - 1s 2ms/step - loss: 7.3436 - mean_absolute_error: 7.3436 - val_loss: 7.7871 - val_mean_absolute_error: 7.7871\n",
      "Epoch 290/500\n",
      "650/660 [============================>.] - ETA: 0s - loss: 7.4796 - mean_absolute_error: 7.4796\n",
      "Epoch 00290: val_loss did not improve from 7.04707\n",
      "660/660 [==============================] - 1s 2ms/step - loss: 7.4598 - mean_absolute_error: 7.4598 - val_loss: 7.2321 - val_mean_absolute_error: 7.2321\n",
      "Epoch 291/500\n",
      "656/660 [============================>.] - ETA: 0s - loss: 7.2980 - mean_absolute_error: 7.2980\n",
      "Epoch 00291: val_loss did not improve from 7.04707\n",
      "660/660 [==============================] - 1s 2ms/step - loss: 7.2965 - mean_absolute_error: 7.2965 - val_loss: 7.1045 - val_mean_absolute_error: 7.1045\n",
      "Epoch 292/500\n",
      "646/660 [============================>.] - ETA: 0s - loss: 7.3121 - mean_absolute_error: 7.3121\n",
      "Epoch 00292: val_loss did not improve from 7.04707\n",
      "660/660 [==============================] - 1s 2ms/step - loss: 7.3143 - mean_absolute_error: 7.3143 - val_loss: 7.5723 - val_mean_absolute_error: 7.5723\n",
      "Epoch 293/500\n",
      "654/660 [============================>.] - ETA: 0s - loss: 7.3502 - mean_absolute_error: 7.3502\n",
      "Epoch 00293: val_loss did not improve from 7.04707\n",
      "660/660 [==============================] - 1s 2ms/step - loss: 7.3528 - mean_absolute_error: 7.3528 - val_loss: 9.5335 - val_mean_absolute_error: 9.5335\n",
      "Epoch 294/500\n",
      "640/660 [============================>.] - ETA: 0s - loss: 7.3408 - mean_absolute_error: 7.3408\n",
      "Epoch 00294: val_loss did not improve from 7.04707\n",
      "660/660 [==============================] - 1s 2ms/step - loss: 7.3436 - mean_absolute_error: 7.3436 - val_loss: 7.3549 - val_mean_absolute_error: 7.3549\n",
      "Epoch 295/500\n",
      "642/660 [============================>.] - ETA: 0s - loss: 7.3969 - mean_absolute_error: 7.3969\n",
      "Epoch 00295: val_loss did not improve from 7.04707\n",
      "660/660 [==============================] - 2s 2ms/step - loss: 7.4157 - mean_absolute_error: 7.4157 - val_loss: 7.3050 - val_mean_absolute_error: 7.3050\n",
      "Epoch 296/500\n",
      "653/660 [============================>.] - ETA: 0s - loss: 7.3755 - mean_absolute_error: 7.3755\n",
      "Epoch 00296: val_loss did not improve from 7.04707\n",
      "660/660 [==============================] - 1s 2ms/step - loss: 7.3880 - mean_absolute_error: 7.3880 - val_loss: 7.5828 - val_mean_absolute_error: 7.5828\n",
      "Epoch 297/500\n",
      "636/660 [===========================>..] - ETA: 0s - loss: 7.3772 - mean_absolute_error: 7.3772\n",
      "Epoch 00297: val_loss did not improve from 7.04707\n",
      "660/660 [==============================] - 1s 2ms/step - loss: 7.3659 - mean_absolute_error: 7.3659 - val_loss: 7.1663 - val_mean_absolute_error: 7.1663\n",
      "Epoch 298/500\n",
      "643/660 [============================>.] - ETA: 0s - loss: 7.2985 - mean_absolute_error: 7.2985\n",
      "Epoch 00298: val_loss did not improve from 7.04707\n",
      "660/660 [==============================] - 1s 2ms/step - loss: 7.3009 - mean_absolute_error: 7.3009 - val_loss: 7.5305 - val_mean_absolute_error: 7.5305\n",
      "Epoch 299/500\n",
      "647/660 [============================>.] - ETA: 0s - loss: 7.3337 - mean_absolute_error: 7.3337\n",
      "Epoch 00299: val_loss did not improve from 7.04707\n",
      "660/660 [==============================] - 1s 2ms/step - loss: 7.3443 - mean_absolute_error: 7.3443 - val_loss: 7.0613 - val_mean_absolute_error: 7.0613\n",
      "Epoch 300/500\n",
      "637/660 [===========================>..] - ETA: 0s - loss: 7.3668 - mean_absolute_error: 7.3668\n",
      "Epoch 00300: val_loss did not improve from 7.04707\n",
      "660/660 [==============================] - 1s 2ms/step - loss: 7.3710 - mean_absolute_error: 7.3710 - val_loss: 7.4481 - val_mean_absolute_error: 7.4481\n",
      "Epoch 301/500\n",
      "645/660 [============================>.] - ETA: 0s - loss: 7.3192 - mean_absolute_error: 7.3192\n",
      "Epoch 00301: val_loss did not improve from 7.04707\n",
      "660/660 [==============================] - 1s 2ms/step - loss: 7.3131 - mean_absolute_error: 7.3131 - val_loss: 7.3081 - val_mean_absolute_error: 7.3081\n",
      "Epoch 302/500\n",
      "642/660 [============================>.] - ETA: 0s - loss: 7.3762 - mean_absolute_error: 7.3762\n",
      "Epoch 00302: val_loss did not improve from 7.04707\n",
      "660/660 [==============================] - 1s 2ms/step - loss: 7.3493 - mean_absolute_error: 7.3493 - val_loss: 7.5594 - val_mean_absolute_error: 7.5594\n",
      "Epoch 303/500\n",
      "656/660 [============================>.] - ETA: 0s - loss: 7.3938 - mean_absolute_error: 7.3938\n",
      "Epoch 00303: val_loss did not improve from 7.04707\n",
      "660/660 [==============================] - 1s 2ms/step - loss: 7.3890 - mean_absolute_error: 7.3890 - val_loss: 7.6189 - val_mean_absolute_error: 7.6189\n",
      "Epoch 304/500\n",
      "654/660 [============================>.] - ETA: 0s - loss: 7.3285 - mean_absolute_error: 7.3285\n",
      "Epoch 00304: val_loss did not improve from 7.04707\n",
      "660/660 [==============================] - 1s 2ms/step - loss: 7.3273 - mean_absolute_error: 7.3273 - val_loss: 7.1415 - val_mean_absolute_error: 7.1415\n",
      "Epoch 305/500\n",
      "640/660 [============================>.] - ETA: 0s - loss: 7.2684 - mean_absolute_error: 7.2684\n",
      "Epoch 00305: val_loss did not improve from 7.04707\n",
      "660/660 [==============================] - 1s 2ms/step - loss: 7.2840 - mean_absolute_error: 7.2840 - val_loss: 7.0926 - val_mean_absolute_error: 7.0926\n",
      "Epoch 306/500\n",
      "659/660 [============================>.] - ETA: 0s - loss: 7.3729 - mean_absolute_error: 7.3729\n",
      "Epoch 00306: val_loss did not improve from 7.04707\n",
      "660/660 [==============================] - 1s 2ms/step - loss: 7.3708 - mean_absolute_error: 7.3708 - val_loss: 7.6281 - val_mean_absolute_error: 7.6281\n",
      "Epoch 307/500\n",
      "652/660 [============================>.] - ETA: 0s - loss: 7.2943 - mean_absolute_error: 7.2943\n",
      "Epoch 00307: val_loss did not improve from 7.04707\n",
      "660/660 [==============================] - 1s 2ms/step - loss: 7.2980 - mean_absolute_error: 7.2980 - val_loss: 7.1405 - val_mean_absolute_error: 7.1405\n",
      "Epoch 308/500\n",
      "655/660 [============================>.] - ETA: 0s - loss: 7.3773 - mean_absolute_error: 7.3773\n",
      "Epoch 00308: val_loss did not improve from 7.04707\n",
      "660/660 [==============================] - 1s 2ms/step - loss: 7.3777 - mean_absolute_error: 7.3777 - val_loss: 7.2286 - val_mean_absolute_error: 7.2286\n",
      "Epoch 309/500\n",
      "656/660 [============================>.] - ETA: 0s - loss: 7.3802 - mean_absolute_error: 7.3802\n",
      "Epoch 00309: val_loss did not improve from 7.04707\n",
      "660/660 [==============================] - 1s 2ms/step - loss: 7.3786 - mean_absolute_error: 7.3786 - val_loss: 7.5423 - val_mean_absolute_error: 7.5423\n",
      "Epoch 310/500\n",
      "644/660 [============================>.] - ETA: 0s - loss: 7.2838 - mean_absolute_error: 7.2838\n",
      "Epoch 00310: val_loss did not improve from 7.04707\n",
      "660/660 [==============================] - 1s 2ms/step - loss: 7.3007 - mean_absolute_error: 7.3007 - val_loss: 7.2167 - val_mean_absolute_error: 7.2167\n",
      "Epoch 311/500\n",
      "652/660 [============================>.] - ETA: 0s - loss: 7.3157 - mean_absolute_error: 7.3157\n",
      "Epoch 00311: val_loss did not improve from 7.04707\n",
      "660/660 [==============================] - 1s 2ms/step - loss: 7.3141 - mean_absolute_error: 7.3141 - val_loss: 7.1213 - val_mean_absolute_error: 7.1213\n",
      "Epoch 312/500\n",
      "644/660 [============================>.] - ETA: 0s - loss: 7.3332 - mean_absolute_error: 7.3332\n",
      "Epoch 00312: val_loss did not improve from 7.04707\n",
      "660/660 [==============================] - 1s 2ms/step - loss: 7.3271 - mean_absolute_error: 7.3271 - val_loss: 7.1756 - val_mean_absolute_error: 7.1756\n",
      "Epoch 313/500\n",
      "658/660 [============================>.] - ETA: 0s - loss: 7.2983 - mean_absolute_error: 7.2983\n",
      "Epoch 00313: val_loss did not improve from 7.04707\n",
      "660/660 [==============================] - 1s 2ms/step - loss: 7.2972 - mean_absolute_error: 7.2972 - val_loss: 7.0742 - val_mean_absolute_error: 7.0742\n",
      "Epoch 314/500\n",
      "636/660 [===========================>..] - ETA: 0s - loss: 7.2881 - mean_absolute_error: 7.2881\n",
      "Epoch 00314: val_loss did not improve from 7.04707\n",
      "660/660 [==============================] - 1s 2ms/step - loss: 7.2793 - mean_absolute_error: 7.2793 - val_loss: 7.1794 - val_mean_absolute_error: 7.1794\n",
      "Epoch 315/500\n",
      "656/660 [============================>.] - ETA: 0s - loss: 7.3058 - mean_absolute_error: 7.3058\n",
      "Epoch 00315: val_loss did not improve from 7.04707\n",
      "660/660 [==============================] - 1s 2ms/step - loss: 7.3169 - mean_absolute_error: 7.3169 - val_loss: 7.2779 - val_mean_absolute_error: 7.2779\n",
      "Epoch 316/500\n",
      "660/660 [==============================] - ETA: 0s - loss: 7.3448 - mean_absolute_error: 7.3448\n",
      "Epoch 00316: val_loss did not improve from 7.04707\n",
      "660/660 [==============================] - 1s 2ms/step - loss: 7.3448 - mean_absolute_error: 7.3448 - val_loss: 8.5244 - val_mean_absolute_error: 8.5244\n",
      "Epoch 317/500\n",
      "636/660 [===========================>..] - ETA: 0s - loss: 7.3591 - mean_absolute_error: 7.3591\n",
      "Epoch 00317: val_loss did not improve from 7.04707\n",
      "660/660 [==============================] - 1s 2ms/step - loss: 7.3517 - mean_absolute_error: 7.3517 - val_loss: 7.3860 - val_mean_absolute_error: 7.3860\n",
      "Epoch 318/500\n",
      "655/660 [============================>.] - ETA: 0s - loss: 7.3123 - mean_absolute_error: 7.3123\n",
      "Epoch 00318: val_loss did not improve from 7.04707\n",
      "660/660 [==============================] - 1s 2ms/step - loss: 7.3160 - mean_absolute_error: 7.3160 - val_loss: 7.1898 - val_mean_absolute_error: 7.1898\n",
      "Epoch 319/500\n",
      "655/660 [============================>.] - ETA: 0s - loss: 7.2608 - mean_absolute_error: 7.2608\n",
      "Epoch 00319: val_loss did not improve from 7.04707\n",
      "660/660 [==============================] - 1s 2ms/step - loss: 7.2624 - mean_absolute_error: 7.2624 - val_loss: 7.2136 - val_mean_absolute_error: 7.2136\n",
      "Epoch 320/500\n",
      "639/660 [============================>.] - ETA: 0s - loss: 7.3609 - mean_absolute_error: 7.3609\n",
      "Epoch 00320: val_loss did not improve from 7.04707\n",
      "660/660 [==============================] - 1s 2ms/step - loss: 7.3605 - mean_absolute_error: 7.3605 - val_loss: 7.2832 - val_mean_absolute_error: 7.2832\n",
      "Epoch 321/500\n",
      "639/660 [============================>.] - ETA: 0s - loss: 7.2230 - mean_absolute_error: 7.2230\n",
      "Epoch 00321: val_loss did not improve from 7.04707\n",
      "660/660 [==============================] - 1s 2ms/step - loss: 7.2476 - mean_absolute_error: 7.2476 - val_loss: 7.1959 - val_mean_absolute_error: 7.1959\n",
      "Epoch 322/500\n",
      "654/660 [============================>.] - ETA: 0s - loss: 7.3077 - mean_absolute_error: 7.3077\n",
      "Epoch 00322: val_loss did not improve from 7.04707\n",
      "660/660 [==============================] - 1s 2ms/step - loss: 7.3098 - mean_absolute_error: 7.3098 - val_loss: 7.3919 - val_mean_absolute_error: 7.3919\n",
      "Epoch 323/500\n",
      "656/660 [============================>.] - ETA: 0s - loss: 7.3194 - mean_absolute_error: 7.3194\n",
      "Epoch 00323: val_loss did not improve from 7.04707\n",
      "660/660 [==============================] - 1s 2ms/step - loss: 7.3223 - mean_absolute_error: 7.3223 - val_loss: 7.1277 - val_mean_absolute_error: 7.1277\n",
      "Epoch 324/500\n",
      "653/660 [============================>.] - ETA: 0s - loss: 7.3207 - mean_absolute_error: 7.3207\n",
      "Epoch 00324: val_loss did not improve from 7.04707\n",
      "660/660 [==============================] - 1s 2ms/step - loss: 7.3108 - mean_absolute_error: 7.3108 - val_loss: 7.3094 - val_mean_absolute_error: 7.3094\n",
      "Epoch 325/500\n",
      "656/660 [============================>.] - ETA: 0s - loss: 7.3000 - mean_absolute_error: 7.3000\n",
      "Epoch 00325: val_loss did not improve from 7.04707\n",
      "660/660 [==============================] - 1s 2ms/step - loss: 7.2949 - mean_absolute_error: 7.2949 - val_loss: 7.3695 - val_mean_absolute_error: 7.3695\n",
      "Epoch 326/500\n",
      "659/660 [============================>.] - ETA: 0s - loss: 7.3847 - mean_absolute_error: 7.3847\n",
      "Epoch 00326: val_loss did not improve from 7.04707\n",
      "660/660 [==============================] - 1s 2ms/step - loss: 7.3879 - mean_absolute_error: 7.3879 - val_loss: 7.0988 - val_mean_absolute_error: 7.0988\n",
      "Epoch 327/500\n",
      "655/660 [============================>.] - ETA: 0s - loss: 7.2891 - mean_absolute_error: 7.2891\n",
      "Epoch 00327: val_loss improved from 7.04707 to 7.04236, saving model to Weights-327--7.04236.hdf5\n",
      "660/660 [==============================] - 2s 2ms/step - loss: 7.2740 - mean_absolute_error: 7.2740 - val_loss: 7.0424 - val_mean_absolute_error: 7.0424\n",
      "Epoch 328/500\n",
      "653/660 [============================>.] - ETA: 0s - loss: 7.3594 - mean_absolute_error: 7.3594\n",
      "Epoch 00328: val_loss did not improve from 7.04236\n",
      "660/660 [==============================] - 1s 2ms/step - loss: 7.3654 - mean_absolute_error: 7.3654 - val_loss: 7.2876 - val_mean_absolute_error: 7.2876\n",
      "Epoch 329/500\n",
      "658/660 [============================>.] - ETA: 0s - loss: 7.3019 - mean_absolute_error: 7.3019\n",
      "Epoch 00329: val_loss did not improve from 7.04236\n",
      "660/660 [==============================] - 1s 2ms/step - loss: 7.2992 - mean_absolute_error: 7.2992 - val_loss: 7.2027 - val_mean_absolute_error: 7.2027\n",
      "Epoch 330/500\n",
      "636/660 [===========================>..] - ETA: 0s - loss: 7.3047 - mean_absolute_error: 7.3047\n",
      "Epoch 00330: val_loss did not improve from 7.04236\n",
      "660/660 [==============================] - 1s 2ms/step - loss: 7.2849 - mean_absolute_error: 7.2849 - val_loss: 7.4176 - val_mean_absolute_error: 7.4176\n",
      "Epoch 331/500\n",
      "641/660 [============================>.] - ETA: 0s - loss: 7.3014 - mean_absolute_error: 7.3014\n",
      "Epoch 00331: val_loss did not improve from 7.04236\n",
      "660/660 [==============================] - 1s 2ms/step - loss: 7.3107 - mean_absolute_error: 7.3107 - val_loss: 7.0932 - val_mean_absolute_error: 7.0932\n",
      "Epoch 332/500\n",
      "655/660 [============================>.] - ETA: 0s - loss: 7.3575 - mean_absolute_error: 7.3575\n",
      "Epoch 00332: val_loss did not improve from 7.04236\n",
      "660/660 [==============================] - 1s 2ms/step - loss: 7.3538 - mean_absolute_error: 7.3538 - val_loss: 7.4273 - val_mean_absolute_error: 7.4273\n",
      "Epoch 333/500\n",
      "646/660 [============================>.] - ETA: 0s - loss: 7.3035 - mean_absolute_error: 7.3035\n",
      "Epoch 00333: val_loss did not improve from 7.04236\n",
      "660/660 [==============================] - 1s 2ms/step - loss: 7.3144 - mean_absolute_error: 7.3144 - val_loss: 8.6698 - val_mean_absolute_error: 8.6698\n",
      "Epoch 334/500\n",
      "660/660 [==============================] - ETA: 0s - loss: 7.2750 - mean_absolute_error: 7.2750\n",
      "Epoch 00334: val_loss did not improve from 7.04236\n",
      "660/660 [==============================] - 1s 2ms/step - loss: 7.2750 - mean_absolute_error: 7.2750 - val_loss: 7.5011 - val_mean_absolute_error: 7.5011\n",
      "Epoch 335/500\n",
      "654/660 [============================>.] - ETA: 0s - loss: 7.2902 - mean_absolute_error: 7.2902\n",
      "Epoch 00335: val_loss did not improve from 7.04236\n",
      "660/660 [==============================] - 1s 2ms/step - loss: 7.2873 - mean_absolute_error: 7.2873 - val_loss: 7.2808 - val_mean_absolute_error: 7.2808\n",
      "Epoch 336/500\n",
      "659/660 [============================>.] - ETA: 0s - loss: 7.2815 - mean_absolute_error: 7.2815\n",
      "Epoch 00336: val_loss improved from 7.04236 to 7.02148, saving model to Weights-336--7.02148.hdf5\n",
      "660/660 [==============================] - 2s 2ms/step - loss: 7.2836 - mean_absolute_error: 7.2836 - val_loss: 7.0215 - val_mean_absolute_error: 7.0215\n",
      "Epoch 337/500\n",
      "650/660 [============================>.] - ETA: 0s - loss: 7.2539 - mean_absolute_error: 7.2539\n",
      "Epoch 00337: val_loss did not improve from 7.02148\n",
      "660/660 [==============================] - 1s 2ms/step - loss: 7.2620 - mean_absolute_error: 7.2620 - val_loss: 7.0745 - val_mean_absolute_error: 7.0745\n",
      "Epoch 338/500\n",
      "649/660 [============================>.] - ETA: 0s - loss: 7.3133 - mean_absolute_error: 7.3133\n",
      "Epoch 00338: val_loss did not improve from 7.02148\n",
      "660/660 [==============================] - 1s 2ms/step - loss: 7.3142 - mean_absolute_error: 7.3142 - val_loss: 7.2272 - val_mean_absolute_error: 7.2272\n",
      "Epoch 339/500\n",
      "644/660 [============================>.] - ETA: 0s - loss: 7.3857 - mean_absolute_error: 7.3857\n",
      "Epoch 00339: val_loss did not improve from 7.02148\n",
      "660/660 [==============================] - 1s 2ms/step - loss: 7.3672 - mean_absolute_error: 7.3672 - val_loss: 7.0326 - val_mean_absolute_error: 7.0326\n",
      "Epoch 340/500\n",
      "652/660 [============================>.] - ETA: 0s - loss: 7.2786 - mean_absolute_error: 7.2786\n",
      "Epoch 00340: val_loss did not improve from 7.02148\n",
      "660/660 [==============================] - 1s 2ms/step - loss: 7.2716 - mean_absolute_error: 7.2716 - val_loss: 7.0755 - val_mean_absolute_error: 7.0755\n",
      "Epoch 341/500\n",
      "658/660 [============================>.] - ETA: 0s - loss: 7.2994 - mean_absolute_error: 7.2994\n",
      "Epoch 00341: val_loss did not improve from 7.02148\n",
      "660/660 [==============================] - 1s 2ms/step - loss: 7.3037 - mean_absolute_error: 7.3037 - val_loss: 7.2187 - val_mean_absolute_error: 7.2187\n",
      "Epoch 342/500\n",
      "657/660 [============================>.] - ETA: 0s - loss: 7.2682 - mean_absolute_error: 7.2682\n",
      "Epoch 00342: val_loss did not improve from 7.02148\n",
      "660/660 [==============================] - 1s 2ms/step - loss: 7.2653 - mean_absolute_error: 7.2653 - val_loss: 7.1520 - val_mean_absolute_error: 7.1520\n",
      "Epoch 343/500\n",
      "636/660 [===========================>..] - ETA: 0s - loss: 7.3297 - mean_absolute_error: 7.3297\n",
      "Epoch 00343: val_loss did not improve from 7.02148\n",
      "660/660 [==============================] - 1s 2ms/step - loss: 7.3200 - mean_absolute_error: 7.3200 - val_loss: 7.3722 - val_mean_absolute_error: 7.3722\n",
      "Epoch 344/500\n",
      "652/660 [============================>.] - ETA: 0s - loss: 7.3946 - mean_absolute_error: 7.3946\n",
      "Epoch 00344: val_loss did not improve from 7.02148\n",
      "660/660 [==============================] - 1s 2ms/step - loss: 7.3890 - mean_absolute_error: 7.3890 - val_loss: 7.8496 - val_mean_absolute_error: 7.8496\n",
      "Epoch 345/500\n",
      "655/660 [============================>.] - ETA: 0s - loss: 7.2974 - mean_absolute_error: 7.2974\n",
      "Epoch 00345: val_loss did not improve from 7.02148\n",
      "660/660 [==============================] - 1s 2ms/step - loss: 7.3011 - mean_absolute_error: 7.3011 - val_loss: 7.3572 - val_mean_absolute_error: 7.3572\n",
      "Epoch 346/500\n",
      "640/660 [============================>.] - ETA: 0s - loss: 7.2869 - mean_absolute_error: 7.2869\n",
      "Epoch 00346: val_loss did not improve from 7.02148\n",
      "660/660 [==============================] - 1s 2ms/step - loss: 7.3124 - mean_absolute_error: 7.3124 - val_loss: 7.1443 - val_mean_absolute_error: 7.1443\n",
      "Epoch 347/500\n",
      "651/660 [============================>.] - ETA: 0s - loss: 7.2850 - mean_absolute_error: 7.2850\n",
      "Epoch 00347: val_loss did not improve from 7.02148\n",
      "660/660 [==============================] - 1s 2ms/step - loss: 7.2900 - mean_absolute_error: 7.2900 - val_loss: 7.3685 - val_mean_absolute_error: 7.3685\n",
      "Epoch 348/500\n",
      "641/660 [============================>.] - ETA: 0s - loss: 7.3170 - mean_absolute_error: 7.3170\n",
      "Epoch 00348: val_loss did not improve from 7.02148\n",
      "660/660 [==============================] - 1s 2ms/step - loss: 7.3006 - mean_absolute_error: 7.3006 - val_loss: 7.1318 - val_mean_absolute_error: 7.1318\n",
      "Epoch 349/500\n",
      "659/660 [============================>.] - ETA: 0s - loss: 7.2404 - mean_absolute_error: 7.2404\n",
      "Epoch 00349: val_loss did not improve from 7.02148\n",
      "660/660 [==============================] - 1s 2ms/step - loss: 7.2404 - mean_absolute_error: 7.2404 - val_loss: 7.7666 - val_mean_absolute_error: 7.7666\n",
      "Epoch 350/500\n",
      "657/660 [============================>.] - ETA: 0s - loss: 7.3419 - mean_absolute_error: 7.3419\n",
      "Epoch 00350: val_loss did not improve from 7.02148\n",
      "660/660 [==============================] - 1s 2ms/step - loss: 7.3431 - mean_absolute_error: 7.3431 - val_loss: 7.5866 - val_mean_absolute_error: 7.5866\n",
      "Epoch 351/500\n",
      "650/660 [============================>.] - ETA: 0s - loss: 7.2897 - mean_absolute_error: 7.2897\n",
      "Epoch 00351: val_loss did not improve from 7.02148\n",
      "660/660 [==============================] - 1s 2ms/step - loss: 7.3091 - mean_absolute_error: 7.3091 - val_loss: 7.5082 - val_mean_absolute_error: 7.5082\n",
      "Epoch 352/500\n",
      "655/660 [============================>.] - ETA: 0s - loss: 7.2936 - mean_absolute_error: 7.2936\n",
      "Epoch 00352: val_loss did not improve from 7.02148\n",
      "660/660 [==============================] - 1s 2ms/step - loss: 7.2979 - mean_absolute_error: 7.2979 - val_loss: 7.2068 - val_mean_absolute_error: 7.2068\n",
      "Epoch 353/500\n",
      "637/660 [===========================>..] - ETA: 0s - loss: 7.2522 - mean_absolute_error: 7.2522\n",
      "Epoch 00353: val_loss did not improve from 7.02148\n",
      "660/660 [==============================] - 1s 2ms/step - loss: 7.2501 - mean_absolute_error: 7.2501 - val_loss: 7.0984 - val_mean_absolute_error: 7.0984\n",
      "Epoch 354/500\n",
      "659/660 [============================>.] - ETA: 0s - loss: 7.2219 - mean_absolute_error: 7.2219\n",
      "Epoch 00354: val_loss did not improve from 7.02148\n",
      "660/660 [==============================] - 1s 2ms/step - loss: 7.2201 - mean_absolute_error: 7.2201 - val_loss: 7.1944 - val_mean_absolute_error: 7.1944\n",
      "Epoch 355/500\n",
      "638/660 [============================>.] - ETA: 0s - loss: 7.3257 - mean_absolute_error: 7.3257\n",
      "Epoch 00355: val_loss did not improve from 7.02148\n",
      "660/660 [==============================] - 1s 2ms/step - loss: 7.3238 - mean_absolute_error: 7.3238 - val_loss: 7.4959 - val_mean_absolute_error: 7.4959\n",
      "Epoch 356/500\n",
      "658/660 [============================>.] - ETA: 0s - loss: 7.3226 - mean_absolute_error: 7.3226\n",
      "Epoch 00356: val_loss did not improve from 7.02148\n",
      "660/660 [==============================] - 1s 2ms/step - loss: 7.3271 - mean_absolute_error: 7.3271 - val_loss: 7.1300 - val_mean_absolute_error: 7.1300\n",
      "Epoch 357/500\n",
      "658/660 [============================>.] - ETA: 0s - loss: 7.2564 - mean_absolute_error: 7.2564\n",
      "Epoch 00357: val_loss did not improve from 7.02148\n",
      "660/660 [==============================] - 1s 2ms/step - loss: 7.2539 - mean_absolute_error: 7.2539 - val_loss: 7.1342 - val_mean_absolute_error: 7.1342\n",
      "Epoch 358/500\n",
      "649/660 [============================>.] - ETA: 0s - loss: 7.3063 - mean_absolute_error: 7.3063\n",
      "Epoch 00358: val_loss did not improve from 7.02148\n",
      "660/660 [==============================] - 1s 2ms/step - loss: 7.3007 - mean_absolute_error: 7.3007 - val_loss: 7.0588 - val_mean_absolute_error: 7.0588\n",
      "Epoch 359/500\n",
      "649/660 [============================>.] - ETA: 0s - loss: 7.2836 - mean_absolute_error: 7.2836\n",
      "Epoch 00359: val_loss did not improve from 7.02148\n",
      "660/660 [==============================] - 1s 2ms/step - loss: 7.2796 - mean_absolute_error: 7.2796 - val_loss: 7.2348 - val_mean_absolute_error: 7.2348\n",
      "Epoch 360/500\n",
      "656/660 [============================>.] - ETA: 0s - loss: 7.3544 - mean_absolute_error: 7.3544\n",
      "Epoch 00360: val_loss did not improve from 7.02148\n",
      "660/660 [==============================] - 1s 2ms/step - loss: 7.3554 - mean_absolute_error: 7.3554 - val_loss: 7.1258 - val_mean_absolute_error: 7.1258\n",
      "Epoch 361/500\n",
      "652/660 [============================>.] - ETA: 0s - loss: 7.2429 - mean_absolute_error: 7.2429\n",
      "Epoch 00361: val_loss did not improve from 7.02148\n",
      "660/660 [==============================] - 1s 2ms/step - loss: 7.2449 - mean_absolute_error: 7.2449 - val_loss: 7.1944 - val_mean_absolute_error: 7.1944\n",
      "Epoch 362/500\n",
      "651/660 [============================>.] - ETA: 0s - loss: 7.2702 - mean_absolute_error: 7.2702\n",
      "Epoch 00362: val_loss did not improve from 7.02148\n",
      "660/660 [==============================] - 1s 2ms/step - loss: 7.2850 - mean_absolute_error: 7.2850 - val_loss: 7.1794 - val_mean_absolute_error: 7.1794\n",
      "Epoch 363/500\n",
      "642/660 [============================>.] - ETA: 0s - loss: 7.3080 - mean_absolute_error: 7.3080\n",
      "Epoch 00363: val_loss did not improve from 7.02148\n",
      "660/660 [==============================] - 1s 2ms/step - loss: 7.3156 - mean_absolute_error: 7.3156 - val_loss: 7.4580 - val_mean_absolute_error: 7.4580\n",
      "Epoch 364/500\n",
      "644/660 [============================>.] - ETA: 0s - loss: 7.2486 - mean_absolute_error: 7.2486\n",
      "Epoch 00364: val_loss did not improve from 7.02148\n",
      "660/660 [==============================] - 1s 2ms/step - loss: 7.2528 - mean_absolute_error: 7.2528 - val_loss: 7.1477 - val_mean_absolute_error: 7.1477\n",
      "Epoch 365/500\n",
      "656/660 [============================>.] - ETA: 0s - loss: 7.2671 - mean_absolute_error: 7.2671\n",
      "Epoch 00365: val_loss did not improve from 7.02148\n",
      "660/660 [==============================] - 1s 2ms/step - loss: 7.2697 - mean_absolute_error: 7.2697 - val_loss: 7.2713 - val_mean_absolute_error: 7.2713\n",
      "Epoch 366/500\n",
      "656/660 [============================>.] - ETA: 0s - loss: 7.2657 - mean_absolute_error: 7.2657\n",
      "Epoch 00366: val_loss did not improve from 7.02148\n",
      "660/660 [==============================] - 1s 2ms/step - loss: 7.2674 - mean_absolute_error: 7.2674 - val_loss: 7.2622 - val_mean_absolute_error: 7.2622\n",
      "Epoch 367/500\n",
      "644/660 [============================>.] - ETA: 0s - loss: 7.2720 - mean_absolute_error: 7.2720\n",
      "Epoch 00367: val_loss did not improve from 7.02148\n",
      "660/660 [==============================] - 1s 2ms/step - loss: 7.3004 - mean_absolute_error: 7.3004 - val_loss: 7.3064 - val_mean_absolute_error: 7.3064\n",
      "Epoch 368/500\n",
      "638/660 [============================>.] - ETA: 0s - loss: 7.1807 - mean_absolute_error: 7.1807\n",
      "Epoch 00368: val_loss did not improve from 7.02148\n",
      "660/660 [==============================] - 1s 2ms/step - loss: 7.2075 - mean_absolute_error: 7.2075 - val_loss: 7.1012 - val_mean_absolute_error: 7.1012\n",
      "Epoch 369/500\n",
      "656/660 [============================>.] - ETA: 0s - loss: 7.2567 - mean_absolute_error: 7.2567\n",
      "Epoch 00369: val_loss did not improve from 7.02148\n",
      "660/660 [==============================] - 1s 2ms/step - loss: 7.2515 - mean_absolute_error: 7.2515 - val_loss: 7.3899 - val_mean_absolute_error: 7.3899\n",
      "Epoch 370/500\n",
      "637/660 [===========================>..] - ETA: 0s - loss: 7.2723 - mean_absolute_error: 7.2723\n",
      "Epoch 00370: val_loss did not improve from 7.02148\n",
      "660/660 [==============================] - 1s 2ms/step - loss: 7.2796 - mean_absolute_error: 7.2796 - val_loss: 7.1100 - val_mean_absolute_error: 7.1100\n",
      "Epoch 371/500\n",
      "654/660 [============================>.] - ETA: 0s - loss: 7.2339 - mean_absolute_error: 7.2339\n",
      "Epoch 00371: val_loss did not improve from 7.02148\n",
      "660/660 [==============================] - 1s 2ms/step - loss: 7.2232 - mean_absolute_error: 7.2232 - val_loss: 7.0855 - val_mean_absolute_error: 7.0855\n",
      "Epoch 372/500\n",
      "638/660 [============================>.] - ETA: 0s - loss: 7.2620 - mean_absolute_error: 7.2620\n",
      "Epoch 00372: val_loss did not improve from 7.02148\n",
      "660/660 [==============================] - 1s 2ms/step - loss: 7.2642 - mean_absolute_error: 7.2642 - val_loss: 7.1679 - val_mean_absolute_error: 7.1679\n",
      "Epoch 373/500\n",
      "651/660 [============================>.] - ETA: 0s - loss: 7.2607 - mean_absolute_error: 7.2607\n",
      "Epoch 00373: val_loss did not improve from 7.02148\n",
      "660/660 [==============================] - 1s 2ms/step - loss: 7.2544 - mean_absolute_error: 7.2544 - val_loss: 7.5693 - val_mean_absolute_error: 7.5693\n",
      "Epoch 374/500\n",
      "638/660 [============================>.] - ETA: 0s - loss: 7.3037 - mean_absolute_error: 7.3037\n",
      "Epoch 00374: val_loss did not improve from 7.02148\n",
      "660/660 [==============================] - 1s 2ms/step - loss: 7.2868 - mean_absolute_error: 7.2868 - val_loss: 7.1222 - val_mean_absolute_error: 7.1222\n",
      "Epoch 375/500\n",
      "643/660 [============================>.] - ETA: 0s - loss: 7.2844 - mean_absolute_error: 7.2844\n",
      "Epoch 00375: val_loss did not improve from 7.02148\n",
      "660/660 [==============================] - 1s 2ms/step - loss: 7.2730 - mean_absolute_error: 7.2730 - val_loss: 7.1556 - val_mean_absolute_error: 7.1556\n",
      "Epoch 376/500\n",
      "657/660 [============================>.] - ETA: 0s - loss: 7.2513 - mean_absolute_error: 7.2513\n",
      "Epoch 00376: val_loss did not improve from 7.02148\n",
      "660/660 [==============================] - 1s 2ms/step - loss: 7.2463 - mean_absolute_error: 7.2463 - val_loss: 7.0709 - val_mean_absolute_error: 7.0709\n",
      "Epoch 377/500\n",
      "657/660 [============================>.] - ETA: 0s - loss: 7.2779 - mean_absolute_error: 7.2779\n",
      "Epoch 00377: val_loss did not improve from 7.02148\n",
      "660/660 [==============================] - 1s 2ms/step - loss: 7.2695 - mean_absolute_error: 7.2695 - val_loss: 7.2250 - val_mean_absolute_error: 7.2250\n",
      "Epoch 378/500\n",
      "646/660 [============================>.] - ETA: 0s - loss: 7.2447 - mean_absolute_error: 7.2447\n",
      "Epoch 00378: val_loss did not improve from 7.02148\n",
      "660/660 [==============================] - 1s 2ms/step - loss: 7.2539 - mean_absolute_error: 7.2539 - val_loss: 7.3325 - val_mean_absolute_error: 7.3325\n",
      "Epoch 379/500\n",
      "656/660 [============================>.] - ETA: 0s - loss: 7.3117 - mean_absolute_error: 7.3117\n",
      "Epoch 00379: val_loss did not improve from 7.02148\n",
      "660/660 [==============================] - 1s 2ms/step - loss: 7.3092 - mean_absolute_error: 7.3092 - val_loss: 7.8281 - val_mean_absolute_error: 7.8281\n",
      "Epoch 380/500\n",
      "653/660 [============================>.] - ETA: 0s - loss: 7.2122 - mean_absolute_error: 7.2122\n",
      "Epoch 00380: val_loss did not improve from 7.02148\n",
      "660/660 [==============================] - 1s 2ms/step - loss: 7.2165 - mean_absolute_error: 7.2165 - val_loss: 7.2203 - val_mean_absolute_error: 7.2203\n",
      "Epoch 381/500\n",
      "658/660 [============================>.] - ETA: 0s - loss: 7.2702 - mean_absolute_error: 7.2702\n",
      "Epoch 00381: val_loss did not improve from 7.02148\n",
      "660/660 [==============================] - 1s 2ms/step - loss: 7.2668 - mean_absolute_error: 7.2668 - val_loss: 7.5140 - val_mean_absolute_error: 7.5140\n",
      "Epoch 382/500\n",
      "658/660 [============================>.] - ETA: 0s - loss: 7.2164 - mean_absolute_error: 7.2164\n",
      "Epoch 00382: val_loss did not improve from 7.02148\n",
      "660/660 [==============================] - 1s 2ms/step - loss: 7.2146 - mean_absolute_error: 7.2146 - val_loss: 7.2153 - val_mean_absolute_error: 7.2153\n",
      "Epoch 383/500\n",
      "651/660 [============================>.] - ETA: 0s - loss: 7.2442 - mean_absolute_error: 7.2442\n",
      "Epoch 00383: val_loss did not improve from 7.02148\n",
      "660/660 [==============================] - 1s 2ms/step - loss: 7.2418 - mean_absolute_error: 7.2418 - val_loss: 7.1257 - val_mean_absolute_error: 7.1257\n",
      "Epoch 384/500\n",
      "640/660 [============================>.] - ETA: 0s - loss: 7.2524 - mean_absolute_error: 7.2524\n",
      "Epoch 00384: val_loss did not improve from 7.02148\n",
      "660/660 [==============================] - 1s 2ms/step - loss: 7.2556 - mean_absolute_error: 7.2556 - val_loss: 7.1559 - val_mean_absolute_error: 7.1559\n",
      "Epoch 385/500\n",
      "637/660 [===========================>..] - ETA: 0s - loss: 7.2719 - mean_absolute_error: 7.2719\n",
      "Epoch 00385: val_loss did not improve from 7.02148\n",
      "660/660 [==============================] - 1s 2ms/step - loss: 7.2787 - mean_absolute_error: 7.2787 - val_loss: 7.1460 - val_mean_absolute_error: 7.1460\n",
      "Epoch 386/500\n",
      "635/660 [===========================>..] - ETA: 0s - loss: 7.2334 - mean_absolute_error: 7.2334\n",
      "Epoch 00386: val_loss did not improve from 7.02148\n",
      "660/660 [==============================] - 1s 2ms/step - loss: 7.2279 - mean_absolute_error: 7.2279 - val_loss: 7.2824 - val_mean_absolute_error: 7.2824\n",
      "Epoch 387/500\n",
      "659/660 [============================>.] - ETA: 0s - loss: 7.2990 - mean_absolute_error: 7.2990\n",
      "Epoch 00387: val_loss did not improve from 7.02148\n",
      "660/660 [==============================] - 1s 2ms/step - loss: 7.2959 - mean_absolute_error: 7.2959 - val_loss: 7.3093 - val_mean_absolute_error: 7.3093\n",
      "Epoch 388/500\n",
      "658/660 [============================>.] - ETA: 0s - loss: 7.2695 - mean_absolute_error: 7.2695\n",
      "Epoch 00388: val_loss did not improve from 7.02148\n",
      "660/660 [==============================] - 1s 2ms/step - loss: 7.2715 - mean_absolute_error: 7.2715 - val_loss: 7.5928 - val_mean_absolute_error: 7.5928\n",
      "Epoch 389/500\n",
      "646/660 [============================>.] - ETA: 0s - loss: 7.2247 - mean_absolute_error: 7.2247\n",
      "Epoch 00389: val_loss did not improve from 7.02148\n",
      "660/660 [==============================] - 1s 2ms/step - loss: 7.2180 - mean_absolute_error: 7.2180 - val_loss: 7.1531 - val_mean_absolute_error: 7.1531\n",
      "Epoch 390/500\n",
      "656/660 [============================>.] - ETA: 0s - loss: 7.2809 - mean_absolute_error: 7.2809\n",
      "Epoch 00390: val_loss did not improve from 7.02148\n",
      "660/660 [==============================] - 1s 2ms/step - loss: 7.2699 - mean_absolute_error: 7.2699 - val_loss: 7.1422 - val_mean_absolute_error: 7.1422\n",
      "Epoch 391/500\n",
      "646/660 [============================>.] - ETA: 0s - loss: 7.3028 - mean_absolute_error: 7.3028\n",
      "Epoch 00391: val_loss did not improve from 7.02148\n",
      "660/660 [==============================] - 1s 2ms/step - loss: 7.3091 - mean_absolute_error: 7.3091 - val_loss: 7.3711 - val_mean_absolute_error: 7.3711\n",
      "Epoch 392/500\n",
      "657/660 [============================>.] - ETA: 0s - loss: 7.2103 - mean_absolute_error: 7.2103\n",
      "Epoch 00392: val_loss did not improve from 7.02148\n",
      "660/660 [==============================] - 1s 2ms/step - loss: 7.2140 - mean_absolute_error: 7.2140 - val_loss: 7.4120 - val_mean_absolute_error: 7.4120\n",
      "Epoch 393/500\n",
      "659/660 [============================>.] - ETA: 0s - loss: 7.2712 - mean_absolute_error: 7.2712\n",
      "Epoch 00393: val_loss did not improve from 7.02148\n",
      "660/660 [==============================] - 1s 2ms/step - loss: 7.2713 - mean_absolute_error: 7.2713 - val_loss: 7.2095 - val_mean_absolute_error: 7.2095\n",
      "Epoch 394/500\n",
      "660/660 [==============================] - ETA: 0s - loss: 7.2283 - mean_absolute_error: 7.2283\n",
      "Epoch 00394: val_loss did not improve from 7.02148\n",
      "660/660 [==============================] - 1s 2ms/step - loss: 7.2283 - mean_absolute_error: 7.2283 - val_loss: 7.2133 - val_mean_absolute_error: 7.2133\n",
      "Epoch 395/500\n",
      "644/660 [============================>.] - ETA: 0s - loss: 7.2595 - mean_absolute_error: 7.2595\n",
      "Epoch 00395: val_loss did not improve from 7.02148\n",
      "660/660 [==============================] - 1s 2ms/step - loss: 7.2558 - mean_absolute_error: 7.2558 - val_loss: 7.4503 - val_mean_absolute_error: 7.4503\n",
      "Epoch 396/500\n",
      "637/660 [===========================>..] - ETA: 0s - loss: 7.2562 - mean_absolute_error: 7.2562\n",
      "Epoch 00396: val_loss did not improve from 7.02148\n",
      "660/660 [==============================] - 1s 2ms/step - loss: 7.2665 - mean_absolute_error: 7.2665 - val_loss: 7.7342 - val_mean_absolute_error: 7.7342\n",
      "Epoch 397/500\n",
      "653/660 [============================>.] - ETA: 0s - loss: 7.2092 - mean_absolute_error: 7.2092\n",
      "Epoch 00397: val_loss did not improve from 7.02148\n",
      "660/660 [==============================] - 1s 2ms/step - loss: 7.2069 - mean_absolute_error: 7.2069 - val_loss: 7.0493 - val_mean_absolute_error: 7.0493\n",
      "Epoch 398/500\n",
      "657/660 [============================>.] - ETA: 0s - loss: 7.2170 - mean_absolute_error: 7.2170\n",
      "Epoch 00398: val_loss did not improve from 7.02148\n",
      "660/660 [==============================] - 1s 2ms/step - loss: 7.2126 - mean_absolute_error: 7.2126 - val_loss: 7.1393 - val_mean_absolute_error: 7.1393\n",
      "Epoch 399/500\n",
      "652/660 [============================>.] - ETA: 0s - loss: 7.1739 - mean_absolute_error: 7.1739\n",
      "Epoch 00399: val_loss did not improve from 7.02148\n",
      "660/660 [==============================] - 1s 2ms/step - loss: 7.1818 - mean_absolute_error: 7.1818 - val_loss: 7.2965 - val_mean_absolute_error: 7.2965\n",
      "Epoch 400/500\n",
      "644/660 [============================>.] - ETA: 0s - loss: 7.2195 - mean_absolute_error: 7.2195"
     ]
    }
   ],
   "source": [
    "NN_model.fit(X_train, y_train, epochs=500, batch_size=32, validation_split = 0.2, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "NN_model.load_weights(\"Weights-453--7.01782.hdf5\") # load it\n",
    "NN_model.compile(loss='mean_absolute_error', optimizer='adam', metrics=['mean_absolute_error'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11308, 1)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred=NN_model.predict_on_batch(X_test)\n",
    "y_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11308, 1)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_arr=np.array(y_test)\n",
    "y_test_arr=y_test_arr.reshape(y_test_arr.shape[0],-1)\n",
    "y_test_arr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff=y_pred-y_test_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:matplotlib.font_manager:generated new fontManager\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as root will break packages and permissions. You should install packages reliably by using venv: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[33mWARNING: You are using pip version 21.1.1; however, version 21.2.4 is available.\n",
      "You should consider upgrading via the '/usr/local/bin/python3.7 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -q matplotlib seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f10801cb850>]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(np.squeeze(NN_model.predict_on_batch(X_test)),np.squeeze(y_test_arr),c='#88c999')\n",
    "plt.xlabel('Input')\n",
    "plt.ylabel('Predicted Output')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[217.24779],\n",
       "       [279.1028 ],\n",
       "       [242.00145],\n",
       "       ...,\n",
       "       [193.9799 ],\n",
       "       [205.30226],\n",
       "       [233.62794]], dtype=float32)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[215.21852135],\n",
       "       [276.0171065 ],\n",
       "       [244.61629816],\n",
       "       ...,\n",
       "       [195.34761703],\n",
       "       [178.00185277],\n",
       "       [206.79461947]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.m5d.16xlarge",
  "kernelspec": {
   "display_name": "Python 3 (TensorFlow 2.3 Python 3.7 GPU Optimized)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/tensorflow-2.3-gpu-py37-cu110-ubuntu18.04-v3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
